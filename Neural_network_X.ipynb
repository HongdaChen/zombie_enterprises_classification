{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.optim as torch_optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from datetime import datetime\n",
    "import pandas_profiling as ppf\n",
    "# from tensorboardX import SummaryWriter\n",
    "# writer=SummaryWriter(logdir='./neuron')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flag</th>\n",
       "      <th>zhaiquanrongzi_chengben</th>\n",
       "      <th>zhaiquanrongzi_edu</th>\n",
       "      <th>neiburongzi_and_maoyirongzi_chengben</th>\n",
       "      <th>neiburongzi_and_maoyirongzi_edu</th>\n",
       "      <th>jinglirun</th>\n",
       "      <th>lirunzonge</th>\n",
       "      <th>suoyouzhe_quanyiheji</th>\n",
       "      <th>nashui_zonge</th>\n",
       "      <th>guquanrognzi_chengben</th>\n",
       "      <th>guquanrognzi_edu</th>\n",
       "      <th>yingye_zongshouru</th>\n",
       "      <th>fuzhai_zonge</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.376463</td>\n",
       "      <td>-0.376167</td>\n",
       "      <td>-0.226258</td>\n",
       "      <td>-0.226812</td>\n",
       "      <td>-0.391587</td>\n",
       "      <td>-0.247471</td>\n",
       "      <td>-0.004741</td>\n",
       "      <td>-0.463019</td>\n",
       "      <td>0.406379</td>\n",
       "      <td>0.405600</td>\n",
       "      <td>-0.338439</td>\n",
       "      <td>-0.399303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.376463</td>\n",
       "      <td>-0.376167</td>\n",
       "      <td>-0.131425</td>\n",
       "      <td>-0.132086</td>\n",
       "      <td>-0.305196</td>\n",
       "      <td>-0.402976</td>\n",
       "      <td>0.106861</td>\n",
       "      <td>-0.463019</td>\n",
       "      <td>-0.232299</td>\n",
       "      <td>-0.232355</td>\n",
       "      <td>-0.547484</td>\n",
       "      <td>-0.371525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.422401</td>\n",
       "      <td>0.422863</td>\n",
       "      <td>-0.325154</td>\n",
       "      <td>-0.325293</td>\n",
       "      <td>-1.181800</td>\n",
       "      <td>0.637125</td>\n",
       "      <td>-0.461944</td>\n",
       "      <td>-0.463019</td>\n",
       "      <td>1.593755</td>\n",
       "      <td>1.592084</td>\n",
       "      <td>1.565990</td>\n",
       "      <td>1.380232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.376463</td>\n",
       "      <td>-0.376167</td>\n",
       "      <td>-0.096611</td>\n",
       "      <td>-0.097367</td>\n",
       "      <td>-0.583686</td>\n",
       "      <td>0.199842</td>\n",
       "      <td>0.356249</td>\n",
       "      <td>-0.463019</td>\n",
       "      <td>0.278852</td>\n",
       "      <td>0.278391</td>\n",
       "      <td>0.119368</td>\n",
       "      <td>-0.410640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.376463</td>\n",
       "      <td>-0.376167</td>\n",
       "      <td>0.276225</td>\n",
       "      <td>0.274462</td>\n",
       "      <td>-1.224502</td>\n",
       "      <td>0.836748</td>\n",
       "      <td>-1.434351</td>\n",
       "      <td>-0.463019</td>\n",
       "      <td>2.129373</td>\n",
       "      <td>2.129724</td>\n",
       "      <td>1.686916</td>\n",
       "      <td>2.114191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999995.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.477889</td>\n",
       "      <td>2.478143</td>\n",
       "      <td>-0.325154</td>\n",
       "      <td>-0.325293</td>\n",
       "      <td>0.220857</td>\n",
       "      <td>1.231584</td>\n",
       "      <td>1.261654</td>\n",
       "      <td>1.235796</td>\n",
       "      <td>0.248468</td>\n",
       "      <td>0.247847</td>\n",
       "      <td>0.656345</td>\n",
       "      <td>0.435555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999996.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.254723</td>\n",
       "      <td>-0.254376</td>\n",
       "      <td>-0.325154</td>\n",
       "      <td>-0.325293</td>\n",
       "      <td>-0.121522</td>\n",
       "      <td>-0.581308</td>\n",
       "      <td>0.239991</td>\n",
       "      <td>-0.356314</td>\n",
       "      <td>-0.176726</td>\n",
       "      <td>-0.176689</td>\n",
       "      <td>-0.696066</td>\n",
       "      <td>-0.730380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999997.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.126307</td>\n",
       "      <td>0.126595</td>\n",
       "      <td>0.714218</td>\n",
       "      <td>0.710454</td>\n",
       "      <td>0.567215</td>\n",
       "      <td>0.303538</td>\n",
       "      <td>0.254127</td>\n",
       "      <td>0.852716</td>\n",
       "      <td>-0.285456</td>\n",
       "      <td>-0.285470</td>\n",
       "      <td>0.119244</td>\n",
       "      <td>-0.294231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999998.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.376463</td>\n",
       "      <td>-0.376167</td>\n",
       "      <td>-0.146600</td>\n",
       "      <td>-0.255767</td>\n",
       "      <td>-0.131595</td>\n",
       "      <td>-0.511365</td>\n",
       "      <td>0.325045</td>\n",
       "      <td>-0.399178</td>\n",
       "      <td>-0.127856</td>\n",
       "      <td>-0.128049</td>\n",
       "      <td>-0.625198</td>\n",
       "      <td>-0.660332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999999.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.103286</td>\n",
       "      <td>-0.102876</td>\n",
       "      <td>-0.325154</td>\n",
       "      <td>-0.325293</td>\n",
       "      <td>-0.168026</td>\n",
       "      <td>-0.570797</td>\n",
       "      <td>0.268431</td>\n",
       "      <td>-0.369331</td>\n",
       "      <td>-0.164113</td>\n",
       "      <td>-0.163797</td>\n",
       "      <td>-0.681008</td>\n",
       "      <td>-0.532487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45931 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          flag zhaiquanrongzi_chengben zhaiquanrongzi_edu  \\\n",
       "ID                                                          \n",
       "28.0       1.0               -0.376463          -0.376167   \n",
       "230.0      1.0               -0.376463          -0.376167   \n",
       "429.0      1.0                0.422401           0.422863   \n",
       "693.0      1.0               -0.376463          -0.376167   \n",
       "727.0      1.0               -0.376463          -0.376167   \n",
       "...        ...                     ...                ...   \n",
       "5999995.0  0.0                2.477889           2.478143   \n",
       "5999996.0  0.0               -0.254723          -0.254376   \n",
       "5999997.0  0.0                0.126307           0.126595   \n",
       "5999998.0  0.0               -0.376463          -0.376167   \n",
       "5999999.0  0.0               -0.103286          -0.102876   \n",
       "\n",
       "          neiburongzi_and_maoyirongzi_chengben  \\\n",
       "ID                                               \n",
       "28.0                                 -0.226258   \n",
       "230.0                                -0.131425   \n",
       "429.0                                -0.325154   \n",
       "693.0                                -0.096611   \n",
       "727.0                                 0.276225   \n",
       "...                                        ...   \n",
       "5999995.0                            -0.325154   \n",
       "5999996.0                            -0.325154   \n",
       "5999997.0                             0.714218   \n",
       "5999998.0                            -0.146600   \n",
       "5999999.0                            -0.325154   \n",
       "\n",
       "          neiburongzi_and_maoyirongzi_edu jinglirun lirunzonge  \\\n",
       "ID                                                               \n",
       "28.0                            -0.226812 -0.391587  -0.247471   \n",
       "230.0                           -0.132086 -0.305196  -0.402976   \n",
       "429.0                           -0.325293 -1.181800   0.637125   \n",
       "693.0                           -0.097367 -0.583686   0.199842   \n",
       "727.0                            0.274462 -1.224502   0.836748   \n",
       "...                                   ...       ...        ...   \n",
       "5999995.0                       -0.325293  0.220857   1.231584   \n",
       "5999996.0                       -0.325293 -0.121522  -0.581308   \n",
       "5999997.0                        0.710454  0.567215   0.303538   \n",
       "5999998.0                       -0.255767 -0.131595  -0.511365   \n",
       "5999999.0                       -0.325293 -0.168026  -0.570797   \n",
       "\n",
       "          suoyouzhe_quanyiheji nashui_zonge guquanrognzi_chengben  \\\n",
       "ID                                                                  \n",
       "28.0                 -0.004741    -0.463019              0.406379   \n",
       "230.0                 0.106861    -0.463019             -0.232299   \n",
       "429.0                -0.461944    -0.463019              1.593755   \n",
       "693.0                 0.356249    -0.463019              0.278852   \n",
       "727.0                -1.434351    -0.463019              2.129373   \n",
       "...                        ...          ...                   ...   \n",
       "5999995.0             1.261654     1.235796              0.248468   \n",
       "5999996.0             0.239991    -0.356314             -0.176726   \n",
       "5999997.0             0.254127     0.852716             -0.285456   \n",
       "5999998.0             0.325045    -0.399178             -0.127856   \n",
       "5999999.0             0.268431    -0.369331             -0.164113   \n",
       "\n",
       "          guquanrognzi_edu yingye_zongshouru fuzhai_zonge  \n",
       "ID                                                         \n",
       "28.0              0.405600         -0.338439    -0.399303  \n",
       "230.0            -0.232355         -0.547484    -0.371525  \n",
       "429.0             1.592084          1.565990     1.380232  \n",
       "693.0             0.278391          0.119368    -0.410640  \n",
       "727.0             2.129724          1.686916     2.114191  \n",
       "...                    ...               ...          ...  \n",
       "5999995.0         0.247847          0.656345     0.435555  \n",
       "5999996.0        -0.176689         -0.696066    -0.730380  \n",
       "5999997.0        -0.285470          0.119244    -0.294231  \n",
       "5999998.0        -0.128049         -0.625198    -0.660332  \n",
       "5999999.0        -0.163797         -0.681008    -0.532487  \n",
       "\n",
       "[45931 rows x 13 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_all_data=pd.read_csv(r'./data/created_data/encoded_all_data.csv').reset_index(drop=True).set_index('ID')\n",
    "encoded_all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making all variables categorical\n",
    "for col in encoded_all_data.columns:\n",
    "    encoded_all_data[col] = encoded_all_data[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# txt=encoded_all_data[['行业','企业类型','控制人类型','区域']]\n",
    "# txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #categorical embedding for columns having more than two values?\n",
    "# embedded_cols = {n: len(col.cat.categories) for n,col in txt.items()}\n",
    "# embedded_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedded_col_names = embedded_cols.keys()\n",
    "# print(embedded_col_names)\n",
    "# len(encoded_all_data.columns) - len(embedded_cols) #number of numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_sizes = [(n_categories, min(50, (n_categories+1)//2)) for _,n_categories in embedded_cols.items()]\n",
    "# embedding_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zhaiquanrongzi_chengben</th>\n",
       "      <th>zhaiquanrongzi_edu</th>\n",
       "      <th>neiburongzi_and_maoyirongzi_chengben</th>\n",
       "      <th>neiburongzi_and_maoyirongzi_edu</th>\n",
       "      <th>jinglirun</th>\n",
       "      <th>lirunzonge</th>\n",
       "      <th>suoyouzhe_quanyiheji</th>\n",
       "      <th>nashui_zonge</th>\n",
       "      <th>guquanrognzi_chengben</th>\n",
       "      <th>guquanrognzi_edu</th>\n",
       "      <th>yingye_zongshouru</th>\n",
       "      <th>fuzhai_zonge</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5992282.0</th>\n",
       "      <td>-0.140242</td>\n",
       "      <td>-0.139879</td>\n",
       "      <td>-0.325154</td>\n",
       "      <td>-0.325293</td>\n",
       "      <td>-0.114495</td>\n",
       "      <td>-0.612358</td>\n",
       "      <td>0.269269</td>\n",
       "      <td>-0.340802</td>\n",
       "      <td>-0.285456</td>\n",
       "      <td>-0.285470</td>\n",
       "      <td>-0.693239</td>\n",
       "      <td>-0.705462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5982045.0</th>\n",
       "      <td>1.966176</td>\n",
       "      <td>1.966437</td>\n",
       "      <td>0.052762</td>\n",
       "      <td>-0.216089</td>\n",
       "      <td>1.596196</td>\n",
       "      <td>1.148906</td>\n",
       "      <td>0.533604</td>\n",
       "      <td>3.938261</td>\n",
       "      <td>-0.285456</td>\n",
       "      <td>-0.285470</td>\n",
       "      <td>2.712321</td>\n",
       "      <td>1.191353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895831.0</th>\n",
       "      <td>0.786722</td>\n",
       "      <td>0.787260</td>\n",
       "      <td>-0.325154</td>\n",
       "      <td>-0.325293</td>\n",
       "      <td>-0.716335</td>\n",
       "      <td>0.826067</td>\n",
       "      <td>0.255958</td>\n",
       "      <td>-0.463019</td>\n",
       "      <td>0.547068</td>\n",
       "      <td>0.546235</td>\n",
       "      <td>0.432168</td>\n",
       "      <td>0.348116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5985243.0</th>\n",
       "      <td>-0.322979</td>\n",
       "      <td>-0.322662</td>\n",
       "      <td>-0.325154</td>\n",
       "      <td>-0.214615</td>\n",
       "      <td>-0.155016</td>\n",
       "      <td>-0.640345</td>\n",
       "      <td>0.219881</td>\n",
       "      <td>-0.380627</td>\n",
       "      <td>-0.283383</td>\n",
       "      <td>-0.283391</td>\n",
       "      <td>-0.729194</td>\n",
       "      <td>-0.748672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5983883.0</th>\n",
       "      <td>-0.042745</td>\n",
       "      <td>-0.042454</td>\n",
       "      <td>-0.218925</td>\n",
       "      <td>-0.219351</td>\n",
       "      <td>-0.306733</td>\n",
       "      <td>-0.338618</td>\n",
       "      <td>0.228194</td>\n",
       "      <td>-0.386355</td>\n",
       "      <td>-0.285456</td>\n",
       "      <td>-0.190409</td>\n",
       "      <td>-0.401226</td>\n",
       "      <td>-0.508057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5984896.0</th>\n",
       "      <td>1.196130</td>\n",
       "      <td>1.196813</td>\n",
       "      <td>-0.325154</td>\n",
       "      <td>-0.325293</td>\n",
       "      <td>1.549562</td>\n",
       "      <td>0.811447</td>\n",
       "      <td>0.434845</td>\n",
       "      <td>1.086850</td>\n",
       "      <td>-0.285456</td>\n",
       "      <td>-0.285470</td>\n",
       "      <td>0.414652</td>\n",
       "      <td>0.372022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5996407.0</th>\n",
       "      <td>-0.376463</td>\n",
       "      <td>-0.376167</td>\n",
       "      <td>0.036678</td>\n",
       "      <td>0.037324</td>\n",
       "      <td>0.111706</td>\n",
       "      <td>-0.215827</td>\n",
       "      <td>0.511119</td>\n",
       "      <td>-0.046966</td>\n",
       "      <td>-0.062900</td>\n",
       "      <td>-0.062541</td>\n",
       "      <td>-0.365487</td>\n",
       "      <td>-0.666507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5994688.0</th>\n",
       "      <td>-0.376463</td>\n",
       "      <td>-0.376167</td>\n",
       "      <td>-0.132856</td>\n",
       "      <td>-0.133514</td>\n",
       "      <td>0.123423</td>\n",
       "      <td>-0.215391</td>\n",
       "      <td>-0.447947</td>\n",
       "      <td>0.100624</td>\n",
       "      <td>0.266530</td>\n",
       "      <td>0.267311</td>\n",
       "      <td>-0.224783</td>\n",
       "      <td>0.548445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5985391.0</th>\n",
       "      <td>-0.376463</td>\n",
       "      <td>-0.376167</td>\n",
       "      <td>-0.289779</td>\n",
       "      <td>-0.290066</td>\n",
       "      <td>-0.171231</td>\n",
       "      <td>-0.619866</td>\n",
       "      <td>0.641145</td>\n",
       "      <td>-0.394550</td>\n",
       "      <td>-0.229778</td>\n",
       "      <td>-0.229855</td>\n",
       "      <td>-0.713523</td>\n",
       "      <td>-0.590810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584259.0</th>\n",
       "      <td>1.838419</td>\n",
       "      <td>1.839643</td>\n",
       "      <td>0.489167</td>\n",
       "      <td>0.485606</td>\n",
       "      <td>-1.149829</td>\n",
       "      <td>2.442112</td>\n",
       "      <td>0.797075</td>\n",
       "      <td>-0.463019</td>\n",
       "      <td>2.046365</td>\n",
       "      <td>2.044501</td>\n",
       "      <td>1.488990</td>\n",
       "      <td>0.891506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24953 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          zhaiquanrongzi_chengben zhaiquanrongzi_edu  \\\n",
       "ID                                                     \n",
       "5992282.0               -0.140242          -0.139879   \n",
       "5982045.0                1.966176           1.966437   \n",
       "895831.0                 0.786722           0.787260   \n",
       "5985243.0               -0.322979          -0.322662   \n",
       "5983883.0               -0.042745          -0.042454   \n",
       "...                           ...                ...   \n",
       "5984896.0                1.196130           1.196813   \n",
       "5996407.0               -0.376463          -0.376167   \n",
       "5994688.0               -0.376463          -0.376167   \n",
       "5985391.0               -0.376463          -0.376167   \n",
       "584259.0                 1.838419           1.839643   \n",
       "\n",
       "          neiburongzi_and_maoyirongzi_chengben  \\\n",
       "ID                                               \n",
       "5992282.0                            -0.325154   \n",
       "5982045.0                             0.052762   \n",
       "895831.0                             -0.325154   \n",
       "5985243.0                            -0.325154   \n",
       "5983883.0                            -0.218925   \n",
       "...                                        ...   \n",
       "5984896.0                            -0.325154   \n",
       "5996407.0                             0.036678   \n",
       "5994688.0                            -0.132856   \n",
       "5985391.0                            -0.289779   \n",
       "584259.0                              0.489167   \n",
       "\n",
       "          neiburongzi_and_maoyirongzi_edu jinglirun lirunzonge  \\\n",
       "ID                                                               \n",
       "5992282.0                       -0.325293 -0.114495  -0.612358   \n",
       "5982045.0                       -0.216089  1.596196   1.148906   \n",
       "895831.0                        -0.325293 -0.716335   0.826067   \n",
       "5985243.0                       -0.214615 -0.155016  -0.640345   \n",
       "5983883.0                       -0.219351 -0.306733  -0.338618   \n",
       "...                                   ...       ...        ...   \n",
       "5984896.0                       -0.325293  1.549562   0.811447   \n",
       "5996407.0                        0.037324  0.111706  -0.215827   \n",
       "5994688.0                       -0.133514  0.123423  -0.215391   \n",
       "5985391.0                       -0.290066 -0.171231  -0.619866   \n",
       "584259.0                         0.485606 -1.149829   2.442112   \n",
       "\n",
       "          suoyouzhe_quanyiheji nashui_zonge guquanrognzi_chengben  \\\n",
       "ID                                                                  \n",
       "5992282.0             0.269269    -0.340802             -0.285456   \n",
       "5982045.0             0.533604     3.938261             -0.285456   \n",
       "895831.0              0.255958    -0.463019              0.547068   \n",
       "5985243.0             0.219881    -0.380627             -0.283383   \n",
       "5983883.0             0.228194    -0.386355             -0.285456   \n",
       "...                        ...          ...                   ...   \n",
       "5984896.0             0.434845     1.086850             -0.285456   \n",
       "5996407.0             0.511119    -0.046966             -0.062900   \n",
       "5994688.0            -0.447947     0.100624              0.266530   \n",
       "5985391.0             0.641145    -0.394550             -0.229778   \n",
       "584259.0              0.797075    -0.463019              2.046365   \n",
       "\n",
       "          guquanrognzi_edu yingye_zongshouru fuzhai_zonge  \n",
       "ID                                                         \n",
       "5992282.0        -0.285470         -0.693239    -0.705462  \n",
       "5982045.0        -0.285470          2.712321     1.191353  \n",
       "895831.0          0.546235          0.432168     0.348116  \n",
       "5985243.0        -0.283391         -0.729194    -0.748672  \n",
       "5983883.0        -0.190409         -0.401226    -0.508057  \n",
       "...                    ...               ...          ...  \n",
       "5984896.0        -0.285470          0.414652     0.372022  \n",
       "5996407.0        -0.062541         -0.365487    -0.666507  \n",
       "5994688.0         0.267311         -0.224783     0.548445  \n",
       "5985391.0        -0.229855         -0.713523    -0.590810  \n",
       "584259.0          2.044501          1.488990     0.891506  \n",
       "\n",
       "[24953 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., ..., 0., 0., 1.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "init_x=encoded_all_data.dropna(subset=['flag']).drop(['flag'],axis=1)\n",
    "init_y=encoded_all_data.dropna(subset=['flag'])['flag'].to_numpy()\n",
    "train_X,val_X, train_y, val_y = train_test_split(init_x,init_y, test_size=0.30, random_state=0)\n",
    "display(train_X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompanyDataset(Dataset):\n",
    "    def __init__(self, X, Y):#, embedded_col_names\n",
    "        X = X.copy()\n",
    "#         self.X1 = X.loc[:,embedded_col_names].copy().values.astype(np.int64) #categorical columnss\n",
    "#         self.X2 = X.drop(columns=embedded_col_names).copy().values.astype(np.float32) #numerical columns\n",
    "        self.x = X.copy().values.astype(np.float32)\n",
    "        self.y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "#         return self.X1[idx], self.X2[idx], self.y[idx]\n",
    "        return self.x[idx],self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating train and valid datasets\n",
    "train_ds = CompanyDataset(train_X, train_y,)#embedded_col_names\n",
    "valid_ds = CompanyDataset(val_X,val_y,)#embedded_col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., ..., 0., 0., 1.])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_X\n",
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlagModel(nn.Module):  #分为两部分categorical 和 continuous\n",
    "    def __init__(self,n):#embedding_sizes, n_cont\n",
    "        super().__init__()\n",
    "#         self.embeddings = nn.ModuleList([nn.Embedding(categories, size) for categories,size in embedding_sizes])\n",
    "#         n_emb = sum(e.embedding_dim for e in self.embeddings) #length of all embeddings combined  \n",
    "        \n",
    "#         self.n_emb, self.n_cont = n_emb, n_cont\n",
    "        \n",
    "        self.lin1 = nn.Linear(n, 200)  #线性层输入： 输出：(200)  self.n_emb + self.n_cont\n",
    "        self.lin2 = nn.Linear(200, 70)\n",
    "        self.lin3 = nn.Linear(70, 2)\n",
    "#         self.bn1 = nn.BatchNorm1d(self.n_cont)\n",
    "        self.bn2 = nn.BatchNorm1d(200)\n",
    "        self.bn3 = nn.BatchNorm1d(70)\n",
    "        \n",
    "#         self.emb_drop = nn.Dropout(0.6)\n",
    "        self.drops = nn.Dropout(0.3)\n",
    "        \n",
    "\n",
    "    def forward(self,x):  #描述了一个前向计算图 x_cat, x_cont\n",
    "#         x = [e(x_cat[:,i]) for i,e in enumerate(self.embeddings)]   #(embeddings): ModuleList(\n",
    "#                                                                                  (0): Embedding(992, 50)\n",
    "#                                                                                  (1): Embedding(51, 26)\n",
    "#   )\n",
    "#         x = torch.cat(x, 1)\n",
    "#         x = self.emb_drop(x)  #把categorical  drop_out\n",
    "        \n",
    "#         x2 = self.bn1(x_cont)   #把continuous标准化\n",
    "        \n",
    "#         x = torch.cat([x, x2], 1)  #把categorical 和 continuous 合并\n",
    "        x = F.relu(self.lin1(x))  #线性层并激活\n",
    "        x = self.drops(x)        #drop_out\n",
    "        \n",
    "        x = self.bn2(x)         #标准化\n",
    "        x = F.relu(self.lin2(x)) #线性层并激活\n",
    "        x = self.drops(x)        #drop_out\n",
    "        x = self.bn3(x)          #标准化\n",
    "        x = self.lin3(x)         #线性层\n",
    "\n",
    "#         x=torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FlagModel(\n",
       "  (lin1): Linear(in_features=12, out_features=200, bias=True)\n",
       "  (lin2): Linear(in_features=200, out_features=70, bias=True)\n",
       "  (lin3): Linear(in_features=70, out_features=2, bias=True)\n",
       "  (bn2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn3): BatchNorm1d(70, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (drops): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FlagModel(12)\n",
    "to_device(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, lr = 0.001, wd = 0.0):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters()) #filter过滤序列，留下True\n",
    "    optim = torch_optim.Adam(parameters, lr=lr, weight_decay=wd)\n",
    "    return optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optim, train_dl,i):\n",
    "    model.train()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    for x1, y in train_dl:\n",
    "        batch = y.shape[0]\n",
    "        output = model(x1)\n",
    "        y=y.long()\n",
    "        loss = F.cross_entropy(output, y)   \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total += batch\n",
    "        sum_loss += batch*(loss.item())\n",
    "#     writer.add_scalar('train_loss',sum_loss/total,i)\n",
    "    return sum_loss/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_loss(model, valid_dl,i):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    correct = 0\n",
    "    for x1,y in valid_dl:\n",
    "        current_batch_size = y.shape[0]\n",
    "        out = model(x1)\n",
    "        y=y.long()\n",
    "        loss = F.cross_entropy(out, y)\n",
    "        sum_loss += current_batch_size*(loss.item())\n",
    "        total += current_batch_size\n",
    "        pred = torch.max(out, 1)[1]\n",
    "        correct += (pred == y).float().sum().item()\n",
    "#         correct = torch.mean((pred == y).float())\n",
    "#     writer.add_scalar('valid_loss',sum_loss/total,i)\n",
    "#     writer.add_scalar('valid_acc',correct/total,i)\n",
    "    print(\"valid loss %f and accuracy %f \" % (sum_loss/total, correct/total))\n",
    "    return sum_loss/total, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, epochs, lr=0.01, wd=0.0):\n",
    "    optim = get_optimizer(model, lr = lr, wd = wd)\n",
    "    for i in range(epochs): \n",
    "        loss = train_model(model, optim, train_dl,i)\n",
    "        print(\"training loss: \", loss)\n",
    "        _,acc=val_loss(model, valid_dl,i)\n",
    "        if acc>0.99:\n",
    "            break\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size,shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DeviceDataLoader(train_dl,device)\n",
    "valid_dl = DeviceDataLoader(valid_dl,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss:  0.7163296199602598\n",
      "valid loss 0.641088 and accuracy 0.613838 \n",
      "training loss:  0.5966178901911269\n",
      "valid loss 0.525381 and accuracy 0.769799 \n",
      "training loss:  0.5200230606234404\n",
      "valid loss 0.451182 and accuracy 0.835531 \n",
      "training loss:  0.4632277373526515\n",
      "valid loss 0.397299 and accuracy 0.870781 \n",
      "training loss:  0.42095123046587135\n",
      "valid loss 0.352325 and accuracy 0.895839 \n",
      "training loss:  0.3824972685762232\n",
      "valid loss 0.312351 and accuracy 0.919776 \n",
      "training loss:  0.3512538374616871\n",
      "valid loss 0.283629 and accuracy 0.927817 \n",
      "training loss:  0.32736037467155016\n",
      "valid loss 0.258092 and accuracy 0.934268 \n",
      "training loss:  0.3067403861082604\n",
      "valid loss 0.237258 and accuracy 0.938943 \n",
      "training loss:  0.2891042162453546\n",
      "valid loss 0.221890 and accuracy 0.941655 \n",
      "training loss:  0.273335045095255\n",
      "valid loss 0.210387 and accuracy 0.942964 \n",
      "training loss:  0.2561964112141851\n",
      "valid loss 0.196228 and accuracy 0.947359 \n",
      "training loss:  0.24681896427774297\n",
      "valid loss 0.185443 and accuracy 0.949509 \n",
      "training loss:  0.23658857059107083\n",
      "valid loss 0.179398 and accuracy 0.949229 \n",
      "training loss:  0.22658116532063352\n",
      "valid loss 0.171056 and accuracy 0.951660 \n",
      "training loss:  0.215122049879594\n",
      "valid loss 0.166292 and accuracy 0.952595 \n",
      "training loss:  0.21063676855919838\n",
      "valid loss 0.161458 and accuracy 0.953343 \n",
      "training loss:  0.20182554275412523\n",
      "valid loss 0.154036 and accuracy 0.956989 \n",
      "training loss:  0.1990003988182015\n",
      "valid loss 0.150416 and accuracy 0.956522 \n",
      "training loss:  0.19251692278234295\n",
      "valid loss 0.144168 and accuracy 0.959327 \n",
      "training loss:  0.18516012916955538\n",
      "valid loss 0.140957 and accuracy 0.959794 \n",
      "training loss:  0.18094943090929935\n",
      "valid loss 0.139076 and accuracy 0.958205 \n",
      "training loss:  0.17583151698406296\n",
      "valid loss 0.135420 and accuracy 0.956054 \n",
      "training loss:  0.1736225567787046\n",
      "valid loss 0.132371 and accuracy 0.959514 \n",
      "training loss:  0.1709994228240775\n",
      "valid loss 0.133600 and accuracy 0.951192 \n",
      "training loss:  0.16677294744151258\n",
      "valid loss 0.128920 and accuracy 0.955026 \n",
      "training loss:  0.1619948590746032\n",
      "valid loss 0.124701 and accuracy 0.960542 \n",
      "training loss:  0.15867340958620538\n",
      "valid loss 0.118660 and accuracy 0.964282 \n",
      "training loss:  0.15462634066887973\n",
      "valid loss 0.118212 and accuracy 0.963721 \n",
      "training loss:  0.15287286731583682\n",
      "valid loss 0.116039 and accuracy 0.964750 \n",
      "training loss:  0.15077001087981723\n",
      "valid loss 0.114694 and accuracy 0.965685 \n",
      "training loss:  0.14509523252943918\n",
      "valid loss 0.113548 and accuracy 0.965311 \n",
      "training loss:  0.14535563674445276\n",
      "valid loss 0.110681 and accuracy 0.967087 \n",
      "training loss:  0.1412457154714927\n",
      "valid loss 0.110390 and accuracy 0.962319 \n",
      "training loss:  0.13866734770857986\n",
      "valid loss 0.106633 and accuracy 0.968677 \n",
      "training loss:  0.13449448728563312\n",
      "valid loss 0.105219 and accuracy 0.970079 \n",
      "training loss:  0.13184125549977507\n",
      "valid loss 0.103062 and accuracy 0.968957 \n",
      "training loss:  0.13152534400158425\n",
      "valid loss 0.107014 and accuracy 0.962038 \n",
      "training loss:  0.13318767877137289\n",
      "valid loss 0.103723 and accuracy 0.966713 \n",
      "training loss:  0.12893070212773214\n",
      "valid loss 0.100787 and accuracy 0.970173 \n",
      "training loss:  0.12603931872364715\n",
      "valid loss 0.103524 and accuracy 0.965030 \n",
      "training loss:  0.1241217666050573\n",
      "valid loss 0.100177 and accuracy 0.968957 \n",
      "training loss:  0.12480686002283714\n",
      "valid loss 0.100156 and accuracy 0.969705 \n",
      "training loss:  0.12184411906512406\n",
      "valid loss 0.097647 and accuracy 0.969051 \n",
      "training loss:  0.11929622455217344\n",
      "valid loss 0.095678 and accuracy 0.971856 \n",
      "training loss:  0.11555278466983304\n",
      "valid loss 0.097719 and accuracy 0.967929 \n",
      "training loss:  0.11672250545142394\n",
      "valid loss 0.096056 and accuracy 0.969986 \n",
      "training loss:  0.11580741502765911\n",
      "valid loss 0.097287 and accuracy 0.968864 \n",
      "training loss:  0.11585102523162179\n",
      "valid loss 0.093530 and accuracy 0.972604 \n",
      "training loss:  0.11585744938150329\n",
      "valid loss 0.092912 and accuracy 0.972043 \n",
      "training loss:  0.11372293041559114\n",
      "valid loss 0.092827 and accuracy 0.971388 \n",
      "training loss:  0.11099896721062057\n",
      "valid loss 0.095154 and accuracy 0.967461 \n",
      "training loss:  0.11035354080128248\n",
      "valid loss 0.092019 and accuracy 0.973072 \n",
      "training loss:  0.10832896605728194\n",
      "valid loss 0.093820 and accuracy 0.968209 \n",
      "training loss:  0.11025259365452576\n",
      "valid loss 0.092695 and accuracy 0.969144 \n",
      "training loss:  0.11110895711477595\n",
      "valid loss 0.100131 and accuracy 0.963441 \n",
      "training loss:  0.10628826637847416\n",
      "valid loss 0.089167 and accuracy 0.972885 \n",
      "training loss:  0.10893002279072646\n",
      "valid loss 0.089522 and accuracy 0.972043 \n",
      "training loss:  0.10880961575278995\n",
      "valid loss 0.089671 and accuracy 0.972324 \n",
      "training loss:  0.10718385480466416\n",
      "valid loss 0.093117 and accuracy 0.966526 \n",
      "training loss:  0.1066681695763503\n",
      "valid loss 0.094591 and accuracy 0.966339 \n",
      "training loss:  0.10567228018697677\n",
      "valid loss 0.087198 and accuracy 0.972324 \n",
      "training loss:  0.10562630560920458\n",
      "valid loss 0.087408 and accuracy 0.973820 \n",
      "training loss:  0.10427688907304757\n",
      "valid loss 0.087001 and accuracy 0.973539 \n",
      "training loss:  0.10397239996015306\n",
      "valid loss 0.086000 and accuracy 0.974381 \n",
      "training loss:  0.10384562421082943\n",
      "valid loss 0.087433 and accuracy 0.972791 \n",
      "training loss:  0.10089422005213358\n",
      "valid loss 0.086989 and accuracy 0.972324 \n",
      "training loss:  0.09769425565467274\n",
      "valid loss 0.087992 and accuracy 0.970360 \n",
      "training loss:  0.09959063314288688\n",
      "valid loss 0.086674 and accuracy 0.971669 \n",
      "training loss:  0.09701306370581834\n",
      "valid loss 0.086894 and accuracy 0.971482 \n",
      "training loss:  0.09867602004446602\n",
      "valid loss 0.086988 and accuracy 0.969612 \n",
      "training loss:  0.0973462709904061\n",
      "valid loss 0.083870 and accuracy 0.973913 \n",
      "training loss:  0.09430486670451638\n",
      "valid loss 0.083442 and accuracy 0.974755 \n",
      "training loss:  0.09707635036673291\n",
      "valid loss 0.081926 and accuracy 0.975316 \n",
      "training loss:  0.09792774046157227\n",
      "valid loss 0.082978 and accuracy 0.972885 \n",
      "training loss:  0.09246729642889694\n",
      "valid loss 0.082716 and accuracy 0.972791 \n",
      "training loss:  0.0943728396782119\n",
      "valid loss 0.081759 and accuracy 0.975503 \n",
      "training loss:  0.09559050862211715\n",
      "valid loss 0.083607 and accuracy 0.975129 \n",
      "training loss:  0.09587989764244997\n",
      "valid loss 0.083240 and accuracy 0.972791 \n",
      "training loss:  0.09313414200100732\n",
      "valid loss 0.081139 and accuracy 0.975035 \n",
      "training loss:  0.09400413827225282\n",
      "valid loss 0.081924 and accuracy 0.975129 \n",
      "training loss:  0.09223920206495627\n",
      "valid loss 0.080296 and accuracy 0.975503 \n",
      "training loss:  0.0933319149082115\n",
      "valid loss 0.079489 and accuracy 0.975970 \n",
      "training loss:  0.09152198069053476\n",
      "valid loss 0.081097 and accuracy 0.975970 \n",
      "training loss:  0.09128568739484834\n",
      "valid loss 0.080832 and accuracy 0.974474 \n",
      "training loss:  0.08989954804094293\n",
      "valid loss 0.079737 and accuracy 0.975877 \n",
      "training loss:  0.08898939551970039\n",
      "valid loss 0.080998 and accuracy 0.974661 \n",
      "training loss:  0.08806710227005984\n",
      "valid loss 0.081494 and accuracy 0.973539 \n",
      "training loss:  0.08638042097921167\n",
      "valid loss 0.077559 and accuracy 0.976157 \n",
      "training loss:  0.08921827708959905\n",
      "valid loss 0.078041 and accuracy 0.975222 \n",
      "training loss:  0.08828858336573282\n",
      "valid loss 0.077850 and accuracy 0.975877 \n",
      "training loss:  0.0873020802558947\n",
      "valid loss 0.078696 and accuracy 0.976718 \n",
      "training loss:  0.08887941951704509\n",
      "valid loss 0.077683 and accuracy 0.976531 \n",
      "training loss:  0.0870342657121705\n",
      "valid loss 0.078027 and accuracy 0.975690 \n",
      "training loss:  0.08699905895067256\n",
      "valid loss 0.076422 and accuracy 0.976625 \n",
      "training loss:  0.08402571946338332\n",
      "valid loss 0.077282 and accuracy 0.975503 \n",
      "training loss:  0.08558280390838781\n",
      "valid loss 0.077973 and accuracy 0.976812 \n",
      "training loss:  0.08696457829457918\n",
      "valid loss 0.076592 and accuracy 0.975970 \n",
      "training loss:  0.08462571046969668\n",
      "valid loss 0.075018 and accuracy 0.976718 \n",
      "training loss:  0.0847806606974859\n",
      "valid loss 0.075766 and accuracy 0.975409 \n",
      "training loss:  0.08321130394682408\n",
      "valid loss 0.075294 and accuracy 0.977466 \n",
      "training loss:  0.08343401201275677\n",
      "valid loss 0.075432 and accuracy 0.976718 \n",
      "training loss:  0.08537417806830382\n",
      "valid loss 0.075719 and accuracy 0.977373 \n",
      "training loss:  0.08147426967119849\n",
      "valid loss 0.076386 and accuracy 0.976344 \n",
      "training loss:  0.08219457539549019\n",
      "valid loss 0.075252 and accuracy 0.976251 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss:  0.08316412713950905\n",
      "valid loss 0.081388 and accuracy 0.971669 \n",
      "training loss:  0.0819436456974234\n",
      "valid loss 0.074537 and accuracy 0.976344 \n",
      "training loss:  0.08009893667157826\n",
      "valid loss 0.074442 and accuracy 0.977653 \n",
      "training loss:  0.08042078757836184\n",
      "valid loss 0.073654 and accuracy 0.977186 \n",
      "training loss:  0.08183997974055891\n",
      "valid loss 0.072829 and accuracy 0.977934 \n",
      "training loss:  0.0796487650102529\n",
      "valid loss 0.072491 and accuracy 0.977747 \n",
      "training loss:  0.07833703323008565\n",
      "valid loss 0.072204 and accuracy 0.978214 \n",
      "training loss:  0.08103154991767843\n",
      "valid loss 0.074219 and accuracy 0.976344 \n",
      "training loss:  0.07932913632505008\n",
      "valid loss 0.070666 and accuracy 0.978121 \n",
      "training loss:  0.07814240558907842\n",
      "valid loss 0.072799 and accuracy 0.977092 \n",
      "training loss:  0.0758460185586133\n",
      "valid loss 0.070387 and accuracy 0.978588 \n",
      "training loss:  0.07581838355138966\n",
      "valid loss 0.071561 and accuracy 0.977092 \n",
      "training loss:  0.0761633147612266\n",
      "valid loss 0.069457 and accuracy 0.978962 \n",
      "training loss:  0.07555524331436116\n",
      "valid loss 0.072278 and accuracy 0.977747 \n",
      "training loss:  0.07660347440860431\n",
      "valid loss 0.070008 and accuracy 0.978401 \n",
      "training loss:  0.07454257650736869\n",
      "valid loss 0.075004 and accuracy 0.976251 \n",
      "training loss:  0.07739240297350392\n",
      "valid loss 0.066225 and accuracy 0.979243 \n",
      "training loss:  0.07585567632616247\n",
      "valid loss 0.070924 and accuracy 0.977092 \n",
      "training loss:  0.07376288214661633\n",
      "valid loss 0.068041 and accuracy 0.977934 \n",
      "training loss:  0.07416967902058892\n",
      "valid loss 0.068185 and accuracy 0.977747 \n",
      "training loss:  0.07146298434228972\n",
      "valid loss 0.068240 and accuracy 0.977653 \n",
      "training loss:  0.07181378027322474\n",
      "valid loss 0.065386 and accuracy 0.979243 \n",
      "training loss:  0.07174801308462693\n",
      "valid loss 0.066703 and accuracy 0.979430 \n",
      "training loss:  0.07267245878798209\n",
      "valid loss 0.065398 and accuracy 0.979804 \n",
      "training loss:  0.07156981263080255\n",
      "valid loss 0.064790 and accuracy 0.979804 \n",
      "training loss:  0.07023099527991339\n",
      "valid loss 0.068897 and accuracy 0.976905 \n",
      "training loss:  0.06932202857860328\n",
      "valid loss 0.064723 and accuracy 0.979804 \n",
      "training loss:  0.06858259406892808\n",
      "valid loss 0.065078 and accuracy 0.979243 \n",
      "training loss:  0.06757219082553492\n",
      "valid loss 0.063009 and accuracy 0.980832 \n",
      "training loss:  0.06962243602290706\n",
      "valid loss 0.063583 and accuracy 0.981113 \n",
      "training loss:  0.06780760156258177\n",
      "valid loss 0.062853 and accuracy 0.979991 \n",
      "training loss:  0.06847057849603516\n",
      "valid loss 0.065717 and accuracy 0.977934 \n",
      "training loss:  0.07210728761370194\n",
      "valid loss 0.062750 and accuracy 0.981674 \n",
      "training loss:  0.0681006333826587\n",
      "valid loss 0.062606 and accuracy 0.979336 \n",
      "training loss:  0.06508988212121206\n",
      "valid loss 0.060138 and accuracy 0.981113 \n",
      "training loss:  0.06539710038987084\n",
      "valid loss 0.062044 and accuracy 0.981487 \n",
      "training loss:  0.06542766353438881\n",
      "valid loss 0.060323 and accuracy 0.982422 \n",
      "training loss:  0.06757682195570848\n",
      "valid loss 0.071038 and accuracy 0.975129 \n",
      "training loss:  0.06655340379878323\n",
      "valid loss 0.059080 and accuracy 0.981393 \n",
      "training loss:  0.06447377270983727\n",
      "valid loss 0.059354 and accuracy 0.981767 \n",
      "training loss:  0.06472589746036975\n",
      "valid loss 0.059092 and accuracy 0.981206 \n",
      "training loss:  0.06288557589670386\n",
      "valid loss 0.057841 and accuracy 0.982328 \n",
      "training loss:  0.063011474489977\n",
      "valid loss 0.058970 and accuracy 0.981580 \n",
      "training loss:  0.06341446654923583\n",
      "valid loss 0.056913 and accuracy 0.984198 \n",
      "training loss:  0.06188984551316477\n",
      "valid loss 0.059520 and accuracy 0.980739 \n",
      "training loss:  0.06259264635467884\n",
      "valid loss 0.055296 and accuracy 0.984292 \n",
      "training loss:  0.06027497577377251\n",
      "valid loss 0.056461 and accuracy 0.982796 \n",
      "training loss:  0.061901962860600006\n",
      "valid loss 0.055324 and accuracy 0.984385 \n",
      "training loss:  0.0607444611754044\n",
      "valid loss 0.054043 and accuracy 0.984292 \n",
      "training loss:  0.06091624266710476\n",
      "valid loss 0.054619 and accuracy 0.986349 \n",
      "training loss:  0.06010012650110724\n",
      "valid loss 0.060365 and accuracy 0.980458 \n",
      "training loss:  0.05913065309940739\n",
      "valid loss 0.053634 and accuracy 0.986349 \n",
      "training loss:  0.05923408849345263\n",
      "valid loss 0.054141 and accuracy 0.984572 \n",
      "training loss:  0.05712848079596063\n",
      "valid loss 0.053831 and accuracy 0.984105 \n",
      "training loss:  0.05656077435717661\n",
      "valid loss 0.050991 and accuracy 0.985414 \n",
      "training loss:  0.05678902612683702\n",
      "valid loss 0.049849 and accuracy 0.987097 \n",
      "training loss:  0.05454430537937218\n",
      "valid loss 0.050576 and accuracy 0.985881 \n",
      "training loss:  0.05535939799091938\n",
      "valid loss 0.049411 and accuracy 0.987377 \n",
      "training loss:  0.05787512385448186\n",
      "valid loss 0.052887 and accuracy 0.983918 \n",
      "training loss:  0.05425972156908901\n",
      "valid loss 0.050422 and accuracy 0.987751 \n",
      "training loss:  0.055805150513975665\n",
      "valid loss 0.051145 and accuracy 0.985040 \n",
      "training loss:  0.052289273568649965\n",
      "valid loss 0.048872 and accuracy 0.987284 \n",
      "training loss:  0.05251732100390558\n",
      "valid loss 0.048820 and accuracy 0.986255 \n",
      "training loss:  0.05702566538170736\n",
      "valid loss 0.048787 and accuracy 0.986723 \n",
      "training loss:  0.05219761421002616\n",
      "valid loss 0.049678 and accuracy 0.986536 \n",
      "training loss:  0.05358387306284815\n",
      "valid loss 0.050652 and accuracy 0.988499 \n",
      "training loss:  0.053063104761242555\n",
      "valid loss 0.045816 and accuracy 0.988967 \n",
      "training loss:  0.05030247444454971\n",
      "valid loss 0.046124 and accuracy 0.988593 \n",
      "training loss:  0.052183121807515845\n",
      "valid loss 0.048585 and accuracy 0.987471 \n",
      "training loss:  0.04992053236900286\n",
      "valid loss 0.047193 and accuracy 0.988219 \n",
      "training loss:  0.05033210778509415\n",
      "valid loss 0.045381 and accuracy 0.988219 \n",
      "training loss:  0.05105480844362396\n",
      "valid loss 0.053313 and accuracy 0.984946 \n",
      "training loss:  0.05285027610173217\n",
      "valid loss 0.048334 and accuracy 0.985788 \n",
      "training loss:  0.05021106119992494\n",
      "valid loss 0.044657 and accuracy 0.989715 \n",
      "training loss:  0.04643138566772164\n",
      "valid loss 0.045093 and accuracy 0.988686 \n",
      "training loss:  0.047562938222809936\n",
      "valid loss 0.046578 and accuracy 0.987471 \n",
      "training loss:  0.04898417160671584\n",
      "valid loss 0.043709 and accuracy 0.990276 \n"
     ]
    }
   ],
   "source": [
    "train_loop(model, epochs=1000, lr=0.0001, wd=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以下为贴标签部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目前最好：  99.0276  lr=0.0001 dw=0.05 adam\n",
    "torch.save(model.state_dict(), \"optim_old.npy\")\n",
    "model.load_state_dict(torch.load(\"optim_old.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zhaiquanrongzi_chengben</th>\n",
       "      <th>zhaiquanrongzi_edu</th>\n",
       "      <th>neiburongzi_and_maoyirongzi_chengben</th>\n",
       "      <th>neiburongzi_and_maoyirongzi_edu</th>\n",
       "      <th>jinglirun</th>\n",
       "      <th>lirunzonge</th>\n",
       "      <th>suoyouzhe_quanyiheji</th>\n",
       "      <th>nashui_zonge</th>\n",
       "      <th>guquanrognzi_chengben</th>\n",
       "      <th>guquanrognzi_edu</th>\n",
       "      <th>yingye_zongshouru</th>\n",
       "      <th>fuzhai_zonge</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1080523.0</th>\n",
       "      <td>0.153910</td>\n",
       "      <td>0.154316</td>\n",
       "      <td>0.334840</td>\n",
       "      <td>0.335179</td>\n",
       "      <td>-0.423658</td>\n",
       "      <td>-0.235871</td>\n",
       "      <td>-0.522061</td>\n",
       "      <td>-0.463019</td>\n",
       "      <td>-0.285456</td>\n",
       "      <td>-0.285470</td>\n",
       "      <td>-0.258610</td>\n",
       "      <td>0.416919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1080756.0</th>\n",
       "      <td>2.037227</td>\n",
       "      <td>2.037933</td>\n",
       "      <td>-0.188487</td>\n",
       "      <td>-0.189200</td>\n",
       "      <td>-0.737513</td>\n",
       "      <td>1.150920</td>\n",
       "      <td>-0.921308</td>\n",
       "      <td>-0.463019</td>\n",
       "      <td>-0.285456</td>\n",
       "      <td>-0.285470</td>\n",
       "      <td>0.495737</td>\n",
       "      <td>1.237598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1080951.0</th>\n",
       "      <td>-0.376463</td>\n",
       "      <td>-0.376167</td>\n",
       "      <td>0.396778</td>\n",
       "      <td>0.394059</td>\n",
       "      <td>-0.603652</td>\n",
       "      <td>0.351831</td>\n",
       "      <td>0.076741</td>\n",
       "      <td>-0.463019</td>\n",
       "      <td>-0.285456</td>\n",
       "      <td>-0.285470</td>\n",
       "      <td>0.175824</td>\n",
       "      <td>0.557640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1080972.0</th>\n",
       "      <td>-0.376463</td>\n",
       "      <td>-0.376167</td>\n",
       "      <td>-0.254944</td>\n",
       "      <td>-0.255188</td>\n",
       "      <td>-0.230101</td>\n",
       "      <td>-0.618374</td>\n",
       "      <td>0.206208</td>\n",
       "      <td>-0.463019</td>\n",
       "      <td>-0.263837</td>\n",
       "      <td>-0.263875</td>\n",
       "      <td>-0.726563</td>\n",
       "      <td>-0.773093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081027.0</th>\n",
       "      <td>-0.376463</td>\n",
       "      <td>-0.376167</td>\n",
       "      <td>-0.325154</td>\n",
       "      <td>-0.325293</td>\n",
       "      <td>-0.415439</td>\n",
       "      <td>-0.323241</td>\n",
       "      <td>0.438912</td>\n",
       "      <td>-0.463019</td>\n",
       "      <td>0.185092</td>\n",
       "      <td>0.184636</td>\n",
       "      <td>-0.283236</td>\n",
       "      <td>-0.388421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999614.0</th>\n",
       "      <td>1.952389</td>\n",
       "      <td>1.953307</td>\n",
       "      <td>-0.325154</td>\n",
       "      <td>-0.325293</td>\n",
       "      <td>0.250238</td>\n",
       "      <td>0.206426</td>\n",
       "      <td>0.982232</td>\n",
       "      <td>0.157481</td>\n",
       "      <td>-0.285456</td>\n",
       "      <td>-0.285470</td>\n",
       "      <td>0.070694</td>\n",
       "      <td>-0.035223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999746.0</th>\n",
       "      <td>-0.376463</td>\n",
       "      <td>-0.376167</td>\n",
       "      <td>0.100278</td>\n",
       "      <td>0.098578</td>\n",
       "      <td>0.038375</td>\n",
       "      <td>-0.239628</td>\n",
       "      <td>-0.104054</td>\n",
       "      <td>0.203587</td>\n",
       "      <td>-0.042700</td>\n",
       "      <td>-0.042990</td>\n",
       "      <td>-0.275084</td>\n",
       "      <td>-0.262263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999945.0</th>\n",
       "      <td>-0.376463</td>\n",
       "      <td>-0.376167</td>\n",
       "      <td>-0.302229</td>\n",
       "      <td>-0.213973</td>\n",
       "      <td>-0.191376</td>\n",
       "      <td>-0.644977</td>\n",
       "      <td>0.255412</td>\n",
       "      <td>-0.449279</td>\n",
       "      <td>-0.276388</td>\n",
       "      <td>-0.276409</td>\n",
       "      <td>-0.761829</td>\n",
       "      <td>-0.810960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999952.0</th>\n",
       "      <td>-0.376463</td>\n",
       "      <td>-0.376167</td>\n",
       "      <td>-0.034825</td>\n",
       "      <td>-0.036087</td>\n",
       "      <td>-0.120922</td>\n",
       "      <td>-0.368322</td>\n",
       "      <td>0.136415</td>\n",
       "      <td>-0.325451</td>\n",
       "      <td>-0.129668</td>\n",
       "      <td>-0.129805</td>\n",
       "      <td>-0.502481</td>\n",
       "      <td>-0.382606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999964.0</th>\n",
       "      <td>-0.376463</td>\n",
       "      <td>-0.376167</td>\n",
       "      <td>-0.325154</td>\n",
       "      <td>-0.325293</td>\n",
       "      <td>0.237081</td>\n",
       "      <td>-0.324532</td>\n",
       "      <td>-0.474946</td>\n",
       "      <td>-0.032688</td>\n",
       "      <td>0.228543</td>\n",
       "      <td>0.229238</td>\n",
       "      <td>-0.408808</td>\n",
       "      <td>0.095954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10283 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          zhaiquanrongzi_chengben zhaiquanrongzi_edu  \\\n",
       "ID                                                     \n",
       "1080523.0                0.153910           0.154316   \n",
       "1080756.0                2.037227           2.037933   \n",
       "1080951.0               -0.376463          -0.376167   \n",
       "1080972.0               -0.376463          -0.376167   \n",
       "1081027.0               -0.376463          -0.376167   \n",
       "...                           ...                ...   \n",
       "5999614.0                1.952389           1.953307   \n",
       "5999746.0               -0.376463          -0.376167   \n",
       "5999945.0               -0.376463          -0.376167   \n",
       "5999952.0               -0.376463          -0.376167   \n",
       "5999964.0               -0.376463          -0.376167   \n",
       "\n",
       "          neiburongzi_and_maoyirongzi_chengben  \\\n",
       "ID                                               \n",
       "1080523.0                             0.334840   \n",
       "1080756.0                            -0.188487   \n",
       "1080951.0                             0.396778   \n",
       "1080972.0                            -0.254944   \n",
       "1081027.0                            -0.325154   \n",
       "...                                        ...   \n",
       "5999614.0                            -0.325154   \n",
       "5999746.0                             0.100278   \n",
       "5999945.0                            -0.302229   \n",
       "5999952.0                            -0.034825   \n",
       "5999964.0                            -0.325154   \n",
       "\n",
       "          neiburongzi_and_maoyirongzi_edu jinglirun lirunzonge  \\\n",
       "ID                                                               \n",
       "1080523.0                        0.335179 -0.423658  -0.235871   \n",
       "1080756.0                       -0.189200 -0.737513   1.150920   \n",
       "1080951.0                        0.394059 -0.603652   0.351831   \n",
       "1080972.0                       -0.255188 -0.230101  -0.618374   \n",
       "1081027.0                       -0.325293 -0.415439  -0.323241   \n",
       "...                                   ...       ...        ...   \n",
       "5999614.0                       -0.325293  0.250238   0.206426   \n",
       "5999746.0                        0.098578  0.038375  -0.239628   \n",
       "5999945.0                       -0.213973 -0.191376  -0.644977   \n",
       "5999952.0                       -0.036087 -0.120922  -0.368322   \n",
       "5999964.0                       -0.325293  0.237081  -0.324532   \n",
       "\n",
       "          suoyouzhe_quanyiheji nashui_zonge guquanrognzi_chengben  \\\n",
       "ID                                                                  \n",
       "1080523.0            -0.522061    -0.463019             -0.285456   \n",
       "1080756.0            -0.921308    -0.463019             -0.285456   \n",
       "1080951.0             0.076741    -0.463019             -0.285456   \n",
       "1080972.0             0.206208    -0.463019             -0.263837   \n",
       "1081027.0             0.438912    -0.463019              0.185092   \n",
       "...                        ...          ...                   ...   \n",
       "5999614.0             0.982232     0.157481             -0.285456   \n",
       "5999746.0            -0.104054     0.203587             -0.042700   \n",
       "5999945.0             0.255412    -0.449279             -0.276388   \n",
       "5999952.0             0.136415    -0.325451             -0.129668   \n",
       "5999964.0            -0.474946    -0.032688              0.228543   \n",
       "\n",
       "          guquanrognzi_edu yingye_zongshouru fuzhai_zonge  \n",
       "ID                                                         \n",
       "1080523.0        -0.285470         -0.258610     0.416919  \n",
       "1080756.0        -0.285470          0.495737     1.237598  \n",
       "1080951.0        -0.285470          0.175824     0.557640  \n",
       "1080972.0        -0.263875         -0.726563    -0.773093  \n",
       "1081027.0         0.184636         -0.283236    -0.388421  \n",
       "...                    ...               ...          ...  \n",
       "5999614.0        -0.285470          0.070694    -0.035223  \n",
       "5999746.0        -0.042990         -0.275084    -0.262263  \n",
       "5999945.0        -0.276409         -0.761829    -0.810960  \n",
       "5999952.0        -0.129805         -0.502481    -0.382606  \n",
       "5999964.0         0.229238         -0.408808     0.095954  \n",
       "\n",
       "[10283 rows x 12 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_label=encoded_all_data.drop(init_x.index)\n",
    "non_label_X=non_label.drop(['flag'],axis=1)\n",
    "estimate_y=np.array([1]*non_label_X.shape[0])\n",
    "non_label_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making all variables categorical\n",
    "for col in non_label.columns:\n",
    "    non_label[col] = non_label[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #categorical embedding for columns having more than two values\n",
    "# embedded_cols = {n: len(col.cat.categories) for n,col in txt.items()}\n",
    "# embedded_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_sizes = [(n_categories, min(50, (n_categories+1)//2)) for _,n_categories in embedded_cols.items()]\n",
    "# embedding_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = CompanyDataset(non_label_X,estimate_y) #,embedded_col_names\n",
    "batch_size2 = non_label_X.shape[0]\n",
    "test_dl = DataLoader(test_ds,batch_size=batch_size2,shuffle=False) #返回的是可迭代对象\n",
    "# first=iter(test_dl)\n",
    "# next(first)\n",
    "test_dl = DeviceDataLoader(test_dl, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_dl):\n",
    "    model.eval()\n",
    "    for x1, y in test_dl:\n",
    "        out = model(x1)\n",
    "        pred = torch.max(out, 1)[1]\n",
    "        return pred.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flag=predict(model,test_dl)\n",
    "flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neuron_final=pd.concat([pd.DataFrame(index=non_label_X.index,data=flag,columns=['flag']),init_x],axis=1)\n",
    "# neuron_final=encoded_all_data.combine_first(neuron_final)\n",
    "neuron_final.to_csv(r'./data/created_data/neuron_final.csv')\n",
    "# neuron_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prf=ppf.ProfileReport(neuron_final)\n",
    "prf.to_file('report.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
