{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.optim as torch_optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from datetime import datetime\n",
    "import pandas_profiling as ppf\n",
    "# from tensorboardX import SummaryWriter\n",
    "# writer=SummaryWriter(logdir='./neuron')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_all_data=pd.read_csv(r'./data/created_data/encoded_all_data.csv').reset_index(drop=True).set_index('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making all variables categorical\n",
    "for col in encoded_all_data.columns:\n",
    "    encoded_all_data[col] = encoded_all_data[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>行业</th>\n",
       "      <th>企业类型</th>\n",
       "      <th>控制人类型</th>\n",
       "      <th>区域</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230.0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429.0</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727.0</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137.0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999995.0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999996.0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999997.0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999998.0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999999.0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44587 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          行业 企业类型 控制人类型 区域\n",
       "ID                        \n",
       "28.0       0    0     0  6\n",
       "230.0      3    0     0  1\n",
       "429.0      2    4     1  3\n",
       "727.0      5    3     0  3\n",
       "1137.0     1    2     0  1\n",
       "...       ..  ...   ... ..\n",
       "5999995.0  2    1     0  5\n",
       "5999996.0  5    0     1  4\n",
       "5999997.0  0    4     0  6\n",
       "5999998.0  3    3     1  5\n",
       "5999999.0  0    3     1  6\n",
       "\n",
       "[44587 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt=encoded_all_data[['行业','企业类型','控制人类型','区域']]\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'行业': 6, '企业类型': 5, '控制人类型': 2, '区域': 7}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#categorical embedding for columns having more than two values?\n",
    "embedded_cols = {n: len(col.cat.categories) for n,col in txt.items()}\n",
    "embedded_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['行业', '企业类型', '控制人类型', '区域'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_col_names = embedded_cols.keys()\n",
    "print(embedded_col_names)\n",
    "len(encoded_all_data.columns) - len(embedded_cols) #number of numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, 3), (5, 3), (2, 1), (7, 4)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_sizes = [(n_categories, min(50, (n_categories+1)//2)) for _,n_categories in embedded_cols.items()]\n",
    "embedding_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>专利</th>\n",
       "      <th>企业类型</th>\n",
       "      <th>区域</th>\n",
       "      <th>商标</th>\n",
       "      <th>控制人持股比例</th>\n",
       "      <th>控制人类型</th>\n",
       "      <th>注册时间</th>\n",
       "      <th>注册资本</th>\n",
       "      <th>著作权</th>\n",
       "      <th>行业</th>\n",
       "      <th>...</th>\n",
       "      <th>项目融资和政策融资成本</th>\n",
       "      <th>从业人数</th>\n",
       "      <th>资产总额</th>\n",
       "      <th>负债总额</th>\n",
       "      <th>营业总收入</th>\n",
       "      <th>主营业务收入</th>\n",
       "      <th>利润总额</th>\n",
       "      <th>净利润</th>\n",
       "      <th>纳税总额</th>\n",
       "      <th>所有者权益合计</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5989257.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.973541</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135151</td>\n",
       "      <td>0.563362</td>\n",
       "      <td>-0.536135</td>\n",
       "      <td>-0.609535</td>\n",
       "      <td>-0.405129</td>\n",
       "      <td>-0.448206</td>\n",
       "      <td>-0.248830</td>\n",
       "      <td>-0.204542</td>\n",
       "      <td>0.124025</td>\n",
       "      <td>0.432645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5991406.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.79</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-0.254808</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.340444</td>\n",
       "      <td>-0.212910</td>\n",
       "      <td>-0.256340</td>\n",
       "      <td>-0.397152</td>\n",
       "      <td>-0.520638</td>\n",
       "      <td>-0.499634</td>\n",
       "      <td>-0.355941</td>\n",
       "      <td>0.082914</td>\n",
       "      <td>-0.157695</td>\n",
       "      <td>0.399039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5987623.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.179153</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.116104</td>\n",
       "      <td>0.063135</td>\n",
       "      <td>-0.270980</td>\n",
       "      <td>-0.065330</td>\n",
       "      <td>-0.117421</td>\n",
       "      <td>-0.086136</td>\n",
       "      <td>-0.252447</td>\n",
       "      <td>0.356294</td>\n",
       "      <td>-0.020507</td>\n",
       "      <td>-0.221159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185734.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.447639</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.575990</td>\n",
       "      <td>-0.932660</td>\n",
       "      <td>0.018246</td>\n",
       "      <td>0.396604</td>\n",
       "      <td>0.095771</td>\n",
       "      <td>-0.094450</td>\n",
       "      <td>0.064934</td>\n",
       "      <td>-0.574421</td>\n",
       "      <td>-0.463028</td>\n",
       "      <td>-0.694906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5978163.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.128591</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.340444</td>\n",
       "      <td>-0.088663</td>\n",
       "      <td>-0.034559</td>\n",
       "      <td>-0.045658</td>\n",
       "      <td>-0.416634</td>\n",
       "      <td>-0.435343</td>\n",
       "      <td>-0.210012</td>\n",
       "      <td>-0.090157</td>\n",
       "      <td>-0.106243</td>\n",
       "      <td>0.044631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5985562.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.81</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.611179</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.340444</td>\n",
       "      <td>-0.556945</td>\n",
       "      <td>0.183051</td>\n",
       "      <td>-0.071368</td>\n",
       "      <td>-0.000516</td>\n",
       "      <td>-0.009069</td>\n",
       "      <td>0.379197</td>\n",
       "      <td>-0.270317</td>\n",
       "      <td>-0.232909</td>\n",
       "      <td>0.345724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997416.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.054965</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179926</td>\n",
       "      <td>-0.310291</td>\n",
       "      <td>-0.098579</td>\n",
       "      <td>-0.079990</td>\n",
       "      <td>0.037919</td>\n",
       "      <td>0.089179</td>\n",
       "      <td>-0.357966</td>\n",
       "      <td>-0.171569</td>\n",
       "      <td>-0.044199</td>\n",
       "      <td>0.027948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5995645.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-0.184687</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.594195</td>\n",
       "      <td>0.544538</td>\n",
       "      <td>-0.037865</td>\n",
       "      <td>0.313914</td>\n",
       "      <td>-0.341026</td>\n",
       "      <td>-0.279906</td>\n",
       "      <td>-0.262372</td>\n",
       "      <td>0.078621</td>\n",
       "      <td>-0.152383</td>\n",
       "      <td>-0.611328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5986066.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.257071</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.272015</td>\n",
       "      <td>-1.008557</td>\n",
       "      <td>-0.660721</td>\n",
       "      <td>-0.721325</td>\n",
       "      <td>-0.393754</td>\n",
       "      <td>-0.371140</td>\n",
       "      <td>-0.221115</td>\n",
       "      <td>-0.294612</td>\n",
       "      <td>-0.416134</td>\n",
       "      <td>0.476450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600900.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.134360</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.340444</td>\n",
       "      <td>0.774375</td>\n",
       "      <td>0.767765</td>\n",
       "      <td>1.004472</td>\n",
       "      <td>-0.274106</td>\n",
       "      <td>-0.214165</td>\n",
       "      <td>-0.053880</td>\n",
       "      <td>-0.417260</td>\n",
       "      <td>-0.463028</td>\n",
       "      <td>-0.858424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24208 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            专利 企业类型 区域   商标 控制人持股比例 控制人类型  注册时间      注册资本  著作权 行业  ...  \\\n",
       "ID                                                                 ...   \n",
       "5989257.0  0.0    1  3  0.0    0.70     1   4.0 -0.973541  0.0  4  ...   \n",
       "5991406.0  0.0    3  6  1.0    0.79     1  11.0 -0.254808  0.0  1  ...   \n",
       "5987623.0  1.0    1  4  1.0    0.90     0  10.0  1.179153  0.0  3  ...   \n",
       "185734.0   1.0    0  5  0.0    0.85     1   5.0 -0.447639  0.0  3  ...   \n",
       "5978163.0  0.0    4  1  1.0    0.85     0   4.0 -0.128591  1.0  3  ...   \n",
       "...        ...  ... ..  ...     ...   ...   ...       ...  ... ..  ...   \n",
       "5985562.0  1.0    1  1  0.0    0.81     1   6.0  0.611179  1.0  1  ...   \n",
       "5997416.0  0.0    4  6  0.0    0.52     1   9.0 -0.054965  0.0  1  ...   \n",
       "5995645.0  1.0    2  3  0.0    0.60     0   8.0 -0.184687  1.0  5  ...   \n",
       "5986066.0  0.0    4  4  1.0    0.75     0  13.0  0.257071  1.0  3  ...   \n",
       "600900.0   1.0    0  6  0.0    0.53     0   4.0  0.134360  0.0  3  ...   \n",
       "\n",
       "          项目融资和政策融资成本      从业人数      资产总额      负债总额     营业总收入    主营业务收入  \\\n",
       "ID                                                                        \n",
       "5989257.0    0.135151  0.563362 -0.536135 -0.609535 -0.405129 -0.448206   \n",
       "5991406.0   -0.340444 -0.212910 -0.256340 -0.397152 -0.520638 -0.499634   \n",
       "5987623.0   -0.116104  0.063135 -0.270980 -0.065330 -0.117421 -0.086136   \n",
       "185734.0     0.575990 -0.932660  0.018246  0.396604  0.095771 -0.094450   \n",
       "5978163.0   -0.340444 -0.088663 -0.034559 -0.045658 -0.416634 -0.435343   \n",
       "...               ...       ...       ...       ...       ...       ...   \n",
       "5985562.0   -0.340444 -0.556945  0.183051 -0.071368 -0.000516 -0.009069   \n",
       "5997416.0    0.179926 -0.310291 -0.098579 -0.079990  0.037919  0.089179   \n",
       "5995645.0    0.594195  0.544538 -0.037865  0.313914 -0.341026 -0.279906   \n",
       "5986066.0   -0.272015 -1.008557 -0.660721 -0.721325 -0.393754 -0.371140   \n",
       "600900.0    -0.340444  0.774375  0.767765  1.004472 -0.274106 -0.214165   \n",
       "\n",
       "               利润总额       净利润      纳税总额   所有者权益合计  \n",
       "ID                                                 \n",
       "5989257.0 -0.248830 -0.204542  0.124025  0.432645  \n",
       "5991406.0 -0.355941  0.082914 -0.157695  0.399039  \n",
       "5987623.0 -0.252447  0.356294 -0.020507 -0.221159  \n",
       "185734.0   0.064934 -0.574421 -0.463028 -0.694906  \n",
       "5978163.0 -0.210012 -0.090157 -0.106243  0.044631  \n",
       "...             ...       ...       ...       ...  \n",
       "5985562.0  0.379197 -0.270317 -0.232909  0.345724  \n",
       "5997416.0 -0.357966 -0.171569 -0.044199  0.027948  \n",
       "5995645.0 -0.262372  0.078621 -0.152383 -0.611328  \n",
       "5986066.0 -0.221115 -0.294612 -0.416134  0.476450  \n",
       "600900.0  -0.053880 -0.417260 -0.463028 -0.858424  \n",
       "\n",
       "[24208 rows x 27 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 1.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "init_x=encoded_all_data.dropna(subset=['flag']).drop(['flag'],axis=1)\n",
    "init_y=encoded_all_data.dropna(subset=['flag'])['flag'].to_numpy()\n",
    "train_X,val_X, train_y, val_y = train_test_split(init_x,init_y, test_size=0.30, random_state=0)\n",
    "display(train_X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompanyDataset(Dataset):\n",
    "    def __init__(self, X, Y, embedded_col_names):\n",
    "        X = X.copy()\n",
    "        self.X1 = X.loc[:,embedded_col_names].copy().values.astype(np.int64) #categorical columnss\n",
    "        self.X2 = X.drop(columns=embedded_col_names).copy().values.astype(np.float32) #numerical columns\n",
    "        self.y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X1[idx], self.X2[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating train and valid datasets\n",
    "train_ds = CompanyDataset(train_X, train_y,embedded_col_names)\n",
    "valid_ds = CompanyDataset(val_X,val_y,embedded_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 1.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_X\n",
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlagModel(nn.Module):  #分为两部分categorical 和 continuous\n",
    "    def __init__(self, embedding_sizes, n_cont):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(categories, size) for categories,size in embedding_sizes])\n",
    "        n_emb = sum(e.embedding_dim for e in self.embeddings) #length of all embeddings combined  \n",
    "        \n",
    "        self.n_emb, self.n_cont = n_emb, n_cont\n",
    "        \n",
    "        self.lin1 = nn.Linear(self.n_emb + self.n_cont, 200)  #线性层输入： 输出：(200)\n",
    "        self.lin2 = nn.Linear(200, 70)\n",
    "        self.lin3 = nn.Linear(70, 2)\n",
    "        self.bn1 = nn.BatchNorm1d(self.n_cont)\n",
    "        self.bn2 = nn.BatchNorm1d(200)\n",
    "        self.bn3 = nn.BatchNorm1d(70)\n",
    "        \n",
    "        self.emb_drop = nn.Dropout(0.6)\n",
    "        self.drops = nn.Dropout(0.3)\n",
    "        \n",
    "\n",
    "    def forward(self, x_cat, x_cont):  #描述了一个前向计算图\n",
    "        x = [e(x_cat[:,i]) for i,e in enumerate(self.embeddings)]   #(embeddings): ModuleList(\n",
    "#                                                                                  (0): Embedding(992, 50)\n",
    "#                                                                                  (1): Embedding(51, 26)\n",
    "#   )\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.emb_drop(x)  #把categorical  drop_out\n",
    "        \n",
    "        x2 = self.bn1(x_cont)   #把continuous标准化\n",
    "        \n",
    "        x = torch.cat([x, x2], 1)  #把categorical 和 continuous 合并\n",
    "        x = F.relu(self.lin1(x))  #线性层并激活\n",
    "        x = self.drops(x)        #drop_out\n",
    "        \n",
    "        x = self.bn2(x)         #标准化\n",
    "        x = F.relu(self.lin2(x)) #线性层并激活\n",
    "        x = self.drops(x)        #drop_out\n",
    "        x = self.bn3(x)          #标准化\n",
    "        x = self.lin3(x)         #线性层\n",
    "\n",
    "#         x=torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FlagModel(\n",
       "  (embeddings): ModuleList(\n",
       "    (0): Embedding(6, 3)\n",
       "    (1): Embedding(5, 3)\n",
       "    (2): Embedding(2, 1)\n",
       "    (3): Embedding(7, 4)\n",
       "  )\n",
       "  (lin1): Linear(in_features=34, out_features=200, bias=True)\n",
       "  (lin2): Linear(in_features=200, out_features=70, bias=True)\n",
       "  (lin3): Linear(in_features=70, out_features=2, bias=True)\n",
       "  (bn1): BatchNorm1d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn3): BatchNorm1d(70, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (emb_drop): Dropout(p=0.6, inplace=False)\n",
       "  (drops): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FlagModel(embedding_sizes, 23)\n",
    "to_device(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, lr = 0.001, wd = 0.0):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters()) #filter过滤序列，留下True\n",
    "    optim = torch_optim.Adam(parameters, lr=lr, weight_decay=wd)\n",
    "    return optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optim, train_dl,i):\n",
    "    model.train()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    for x1, x2, y in train_dl:\n",
    "        batch = y.shape[0]\n",
    "        output = model(x1, x2)\n",
    "        y=y.long()\n",
    "        loss = F.cross_entropy(output, y)   \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total += batch\n",
    "        sum_loss += batch*(loss.item())\n",
    "#     writer.add_scalar('train_loss',sum_loss/total,i)\n",
    "    return sum_loss/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_loss(model, valid_dl,i):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    correct = 0\n",
    "    for x1, x2, y in valid_dl:\n",
    "        current_batch_size = y.shape[0]\n",
    "        out = model(x1, x2)\n",
    "        y=y.long()\n",
    "        loss = F.cross_entropy(out, y)\n",
    "        sum_loss += current_batch_size*(loss.item())\n",
    "        total += current_batch_size\n",
    "        pred = torch.max(out, 1)[1]\n",
    "        correct += (pred == y).float().sum().item()\n",
    "#         correct = torch.mean((pred == y).float())\n",
    "#     writer.add_scalar('valid_loss',sum_loss/total,i)\n",
    "#     writer.add_scalar('valid_acc',correct/total,i)\n",
    "    print(\"valid loss %f and accuracy %f \" % (sum_loss/total, correct/total))\n",
    "    return sum_loss/total, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, epochs, lr=0.01, wd=0.0):\n",
    "    optim = get_optimizer(model, lr = lr, wd = wd)\n",
    "    for i in range(epochs): \n",
    "        loss = train_model(model, optim, train_dl,i)\n",
    "        print(\"training loss: \", loss)\n",
    "        _,acc=val_loss(model, valid_dl,i)\n",
    "        if acc>0.987:\n",
    "            break\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size,shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DeviceDataLoader(train_dl,device)\n",
    "valid_dl = DeviceDataLoader(valid_dl,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss:  0.7320813578402688\n",
      "valid loss 0.684440 and accuracy 0.525443 \n",
      "training loss:  0.6695158955125878\n",
      "valid loss 0.622389 and accuracy 0.645528 \n",
      "training loss:  0.6279207590551307\n",
      "valid loss 0.584613 and accuracy 0.688801 \n",
      "training loss:  0.594572888764812\n",
      "valid loss 0.553409 and accuracy 0.725713 \n",
      "training loss:  0.5655740384266066\n",
      "valid loss 0.521922 and accuracy 0.766191 \n",
      "training loss:  0.5423942020375919\n",
      "valid loss 0.498335 and accuracy 0.786527 \n",
      "training loss:  0.5243165813095902\n",
      "valid loss 0.473491 and accuracy 0.808597 \n",
      "training loss:  0.4985568637160064\n",
      "valid loss 0.447596 and accuracy 0.832305 \n",
      "training loss:  0.48472217305440896\n",
      "valid loss 0.423317 and accuracy 0.855243 \n",
      "training loss:  0.46740592936372727\n",
      "valid loss 0.409965 and accuracy 0.856881 \n",
      "training loss:  0.4479378347395275\n",
      "valid loss 0.382522 and accuracy 0.878470 \n",
      "training loss:  0.43339508976750396\n",
      "valid loss 0.362330 and accuracy 0.887336 \n",
      "training loss:  0.41192162821841255\n",
      "valid loss 0.338494 and accuracy 0.900732 \n",
      "training loss:  0.39826212129053973\n",
      "valid loss 0.324267 and accuracy 0.900058 \n",
      "training loss:  0.3847467817774846\n",
      "valid loss 0.303072 and accuracy 0.911527 \n",
      "training loss:  0.37012707411021645\n",
      "valid loss 0.289856 and accuracy 0.915960 \n",
      "training loss:  0.35863485700904496\n",
      "valid loss 0.275356 and accuracy 0.921453 \n",
      "training loss:  0.3431681463912238\n",
      "valid loss 0.261618 and accuracy 0.923285 \n",
      "training loss:  0.3356178408415997\n",
      "valid loss 0.248784 and accuracy 0.929260 \n",
      "training loss:  0.32315361262431463\n",
      "valid loss 0.238593 and accuracy 0.929645 \n",
      "training loss:  0.3146647661333437\n",
      "valid loss 0.230306 and accuracy 0.933211 \n",
      "training loss:  0.3051010001123306\n",
      "valid loss 0.224047 and accuracy 0.933982 \n",
      "training loss:  0.2981872899951481\n",
      "valid loss 0.215494 and accuracy 0.937452 \n",
      "training loss:  0.2903462909876079\n",
      "valid loss 0.208412 and accuracy 0.938608 \n",
      "training loss:  0.2806953801985852\n",
      "valid loss 0.206628 and accuracy 0.940150 \n",
      "training loss:  0.276230275985506\n",
      "valid loss 0.198309 and accuracy 0.941403 \n",
      "training loss:  0.26709802701862456\n",
      "valid loss 0.193097 and accuracy 0.944005 \n",
      "training loss:  0.2664800562599756\n",
      "valid loss 0.190031 and accuracy 0.943427 \n",
      "training loss:  0.258423685505543\n",
      "valid loss 0.188327 and accuracy 0.942078 \n",
      "training loss:  0.24903761729923055\n",
      "valid loss 0.180671 and accuracy 0.946993 \n",
      "training loss:  0.24898914251198298\n",
      "valid loss 0.177822 and accuracy 0.946800 \n",
      "training loss:  0.2367630516919409\n",
      "valid loss 0.171876 and accuracy 0.950655 \n",
      "training loss:  0.23546756606029504\n",
      "valid loss 0.173431 and accuracy 0.949788 \n",
      "training loss:  0.22972147582346292\n",
      "valid loss 0.169174 and accuracy 0.948535 \n",
      "training loss:  0.22985360442961753\n",
      "valid loss 0.163795 and accuracy 0.951523 \n",
      "training loss:  0.2269403515315891\n",
      "valid loss 0.161342 and accuracy 0.952487 \n",
      "training loss:  0.21915570565272227\n",
      "valid loss 0.160783 and accuracy 0.951715 \n",
      "training loss:  0.21320893241724115\n",
      "valid loss 0.152858 and accuracy 0.955281 \n",
      "training loss:  0.21256217732286894\n",
      "valid loss 0.151259 and accuracy 0.955763 \n",
      "training loss:  0.20425609121516455\n",
      "valid loss 0.145910 and accuracy 0.958076 \n",
      "training loss:  0.20079509755848893\n",
      "valid loss 0.142878 and accuracy 0.957498 \n",
      "training loss:  0.19282909381275373\n",
      "valid loss 0.139934 and accuracy 0.960389 \n",
      "training loss:  0.19262342180094022\n",
      "valid loss 0.138982 and accuracy 0.958173 \n",
      "training loss:  0.18728697241357203\n",
      "valid loss 0.136199 and accuracy 0.956823 \n",
      "training loss:  0.1806470007836937\n",
      "valid loss 0.133304 and accuracy 0.959233 \n",
      "training loss:  0.17379455872603938\n",
      "valid loss 0.129490 and accuracy 0.960197 \n",
      "training loss:  0.1698689679298193\n",
      "valid loss 0.127039 and accuracy 0.962124 \n",
      "training loss:  0.16601029613596355\n",
      "valid loss 0.123769 and accuracy 0.959811 \n",
      "training loss:  0.16130425271338686\n",
      "valid loss 0.119628 and accuracy 0.964726 \n",
      "training loss:  0.15680869276741838\n",
      "valid loss 0.117423 and accuracy 0.963955 \n",
      "training loss:  0.15582433448807734\n",
      "valid loss 0.115642 and accuracy 0.966268 \n",
      "training loss:  0.14733894520684812\n",
      "valid loss 0.113616 and accuracy 0.965015 \n",
      "training loss:  0.13891010698465345\n",
      "valid loss 0.111917 and accuracy 0.963666 \n",
      "training loss:  0.13994830666800565\n",
      "valid loss 0.108506 and accuracy 0.965594 \n",
      "training loss:  0.13785644935817018\n",
      "valid loss 0.107968 and accuracy 0.964726 \n",
      "training loss:  0.13769541766989413\n",
      "valid loss 0.106513 and accuracy 0.966461 \n",
      "training loss:  0.13219386378828848\n",
      "valid loss 0.104333 and accuracy 0.968099 \n",
      "training loss:  0.12974550528728795\n",
      "valid loss 0.105091 and accuracy 0.965305 \n",
      "training loss:  0.13248850279644633\n",
      "valid loss 0.104975 and accuracy 0.965690 \n",
      "training loss:  0.1277298067289516\n",
      "valid loss 0.103612 and accuracy 0.965305 \n",
      "training loss:  0.12943422974081759\n",
      "valid loss 0.105218 and accuracy 0.968196 \n",
      "training loss:  0.13150596745947457\n",
      "valid loss 0.102844 and accuracy 0.967521 \n",
      "training loss:  0.12497106299139141\n",
      "valid loss 0.101715 and accuracy 0.969256 \n",
      "training loss:  0.12745431581141534\n",
      "valid loss 0.107843 and accuracy 0.960968 \n",
      "training loss:  0.11935190085791943\n",
      "valid loss 0.111266 and accuracy 0.957594 \n",
      "training loss:  0.12075961473036584\n",
      "valid loss 0.105120 and accuracy 0.962702 \n",
      "training loss:  0.12188244814647796\n",
      "valid loss 0.111157 and accuracy 0.957691 \n",
      "training loss:  0.11927702685390887\n",
      "valid loss 0.098981 and accuracy 0.967039 \n",
      "training loss:  0.12363223811046462\n",
      "valid loss 0.104410 and accuracy 0.964919 \n",
      "training loss:  0.12051585726858524\n",
      "valid loss 0.112619 and accuracy 0.956823 \n",
      "training loss:  0.11729656496458864\n",
      "valid loss 0.107946 and accuracy 0.960293 \n",
      "training loss:  0.11402359103534053\n",
      "valid loss 0.098438 and accuracy 0.970123 \n",
      "training loss:  0.11712216602371217\n",
      "valid loss 0.097291 and accuracy 0.971087 \n",
      "training loss:  0.11535347263179153\n",
      "valid loss 0.098731 and accuracy 0.970220 \n",
      "training loss:  0.11568104181997602\n",
      "valid loss 0.098111 and accuracy 0.971087 \n",
      "training loss:  0.11379230825349895\n",
      "valid loss 0.101293 and accuracy 0.968099 \n",
      "training loss:  0.11008133918972735\n",
      "valid loss 0.099002 and accuracy 0.970316 \n",
      "training loss:  0.11196954213747581\n",
      "valid loss 0.096055 and accuracy 0.971569 \n",
      "training loss:  0.11020168988557344\n",
      "valid loss 0.101252 and accuracy 0.964148 \n",
      "training loss:  0.11226582873836702\n",
      "valid loss 0.110626 and accuracy 0.957787 \n",
      "training loss:  0.10910063886969434\n",
      "valid loss 0.095582 and accuracy 0.970412 \n",
      "training loss:  0.10640289767670537\n",
      "valid loss 0.099315 and accuracy 0.963859 \n",
      "training loss:  0.10625658364522086\n",
      "valid loss 0.094945 and accuracy 0.969931 \n",
      "training loss:  0.10634494511815624\n",
      "valid loss 0.098805 and accuracy 0.966076 \n",
      "training loss:  0.10556112765842372\n",
      "valid loss 0.100100 and accuracy 0.965015 \n",
      "training loss:  0.10848816333773194\n",
      "valid loss 0.099427 and accuracy 0.963763 \n",
      "training loss:  0.10931644260006793\n",
      "valid loss 0.097375 and accuracy 0.967810 \n",
      "training loss:  0.10278839100415708\n",
      "valid loss 0.094349 and accuracy 0.970991 \n",
      "training loss:  0.10396126712631171\n",
      "valid loss 0.094320 and accuracy 0.972244 \n",
      "training loss:  0.1085229232459177\n",
      "valid loss 0.095141 and accuracy 0.967907 \n",
      "training loss:  0.10427181788345323\n",
      "valid loss 0.104073 and accuracy 0.964630 \n",
      "training loss:  0.10049712522508132\n",
      "valid loss 0.095003 and accuracy 0.971087 \n",
      "training loss:  0.1058625999422004\n",
      "valid loss 0.095826 and accuracy 0.969352 \n",
      "training loss:  0.10453270877276283\n",
      "valid loss 0.096222 and accuracy 0.971569 \n",
      "training loss:  0.10593097109374666\n",
      "valid loss 0.098555 and accuracy 0.969738 \n",
      "training loss:  0.10925814882167033\n",
      "valid loss 0.097994 and accuracy 0.968967 \n",
      "training loss:  0.1057139258617529\n",
      "valid loss 0.097079 and accuracy 0.967810 \n",
      "training loss:  0.10030823874595696\n",
      "valid loss 0.093521 and accuracy 0.969738 \n",
      "training loss:  0.10653527346190363\n",
      "valid loss 0.096999 and accuracy 0.970605 \n",
      "training loss:  0.10335797025167777\n",
      "valid loss 0.113406 and accuracy 0.956149 \n",
      "training loss:  0.10291639554406032\n",
      "valid loss 0.095884 and accuracy 0.969834 \n",
      "training loss:  0.10895372043017197\n",
      "valid loss 0.103909 and accuracy 0.963666 \n",
      "training loss:  0.09977804641907213\n",
      "valid loss 0.095711 and accuracy 0.971473 \n",
      "training loss:  0.10406995697903539\n",
      "valid loss 0.098545 and accuracy 0.970412 \n",
      "training loss:  0.10051416038669109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss 0.094774 and accuracy 0.970316 \n",
      "training loss:  0.0997064634985232\n",
      "valid loss 0.093471 and accuracy 0.971762 \n",
      "training loss:  0.10074259869394835\n",
      "valid loss 0.098800 and accuracy 0.969931 \n",
      "training loss:  0.09749359002500595\n",
      "valid loss 0.094115 and accuracy 0.969931 \n",
      "training loss:  0.10150973591399523\n",
      "valid loss 0.095059 and accuracy 0.970894 \n",
      "training loss:  0.09829038267946566\n",
      "valid loss 0.094764 and accuracy 0.969738 \n",
      "training loss:  0.10078439678595465\n",
      "valid loss 0.093960 and accuracy 0.971569 \n",
      "training loss:  0.09759428823380303\n",
      "valid loss 0.094530 and accuracy 0.970605 \n",
      "training loss:  0.0990275999153204\n",
      "valid loss 0.095745 and accuracy 0.968774 \n",
      "training loss:  0.09774179994981178\n",
      "valid loss 0.095100 and accuracy 0.970991 \n",
      "training loss:  0.09585771050060418\n",
      "valid loss 0.095841 and accuracy 0.969352 \n",
      "training loss:  0.10866428776265609\n",
      "valid loss 0.096243 and accuracy 0.970702 \n",
      "training loss:  0.09964475348119065\n",
      "valid loss 0.100510 and accuracy 0.965112 \n",
      "training loss:  0.09197027459789882\n",
      "valid loss 0.095987 and accuracy 0.966750 \n",
      "training loss:  0.09456044222029407\n",
      "valid loss 0.094029 and accuracy 0.971184 \n",
      "training loss:  0.09424452949499974\n",
      "valid loss 0.092121 and accuracy 0.971955 \n",
      "training loss:  0.09586913242286117\n",
      "valid loss 0.092657 and accuracy 0.972629 \n",
      "training loss:  0.09772061333995896\n",
      "valid loss 0.096874 and accuracy 0.966172 \n",
      "training loss:  0.09835999194835482\n",
      "valid loss 0.104199 and accuracy 0.961546 \n",
      "training loss:  0.09761873522855868\n",
      "valid loss 0.100090 and accuracy 0.964244 \n",
      "training loss:  0.0969783698515788\n",
      "valid loss 0.094354 and accuracy 0.969352 \n",
      "training loss:  0.09846681050380097\n",
      "valid loss 0.094017 and accuracy 0.971087 \n",
      "training loss:  0.10533783395737115\n",
      "valid loss 0.098422 and accuracy 0.969545 \n",
      "training loss:  0.09497190824533171\n",
      "valid loss 0.094732 and accuracy 0.970991 \n",
      "training loss:  0.09341486715523911\n",
      "valid loss 0.092268 and accuracy 0.970316 \n",
      "training loss:  0.09203378295049475\n",
      "valid loss 0.091100 and accuracy 0.971762 \n",
      "training loss:  0.09541176161522591\n",
      "valid loss 0.095828 and accuracy 0.970412 \n",
      "training loss:  0.09473180072351316\n",
      "valid loss 0.092359 and accuracy 0.971665 \n",
      "training loss:  0.09058465585801985\n",
      "valid loss 0.091069 and accuracy 0.969738 \n",
      "training loss:  0.09529063255776653\n",
      "valid loss 0.094275 and accuracy 0.969545 \n",
      "training loss:  0.09207412091829252\n",
      "valid loss 0.095666 and accuracy 0.969449 \n",
      "training loss:  0.09652378757023213\n",
      "valid loss 0.095217 and accuracy 0.970027 \n",
      "training loss:  0.08857881089196012\n",
      "valid loss 0.092310 and accuracy 0.970412 \n",
      "training loss:  0.08632201927431514\n",
      "valid loss 0.089573 and accuracy 0.972726 \n",
      "training loss:  0.09196296209926173\n",
      "valid loss 0.091445 and accuracy 0.972533 \n",
      "training loss:  0.09694699413941463\n",
      "valid loss 0.092326 and accuracy 0.971955 \n",
      "training loss:  0.09146051990054178\n",
      "valid loss 0.092411 and accuracy 0.970509 \n",
      "training loss:  0.09475940828853621\n",
      "valid loss 0.095537 and accuracy 0.971184 \n",
      "training loss:  0.09154675994745486\n",
      "valid loss 0.092373 and accuracy 0.971762 \n",
      "training loss:  0.09202433175974269\n",
      "valid loss 0.093416 and accuracy 0.970412 \n",
      "training loss:  0.09353132729381028\n",
      "valid loss 0.091581 and accuracy 0.969352 \n",
      "training loss:  0.0938068886274584\n",
      "valid loss 0.099105 and accuracy 0.964534 \n",
      "training loss:  0.08937199591300146\n",
      "valid loss 0.096601 and accuracy 0.964919 \n",
      "training loss:  0.09157370863591924\n",
      "valid loss 0.093242 and accuracy 0.970991 \n",
      "training loss:  0.0950518146265601\n",
      "valid loss 0.092650 and accuracy 0.971665 \n",
      "training loss:  0.09058375432545668\n",
      "valid loss 0.096732 and accuracy 0.965015 \n",
      "training loss:  0.08757436717444907\n",
      "valid loss 0.088687 and accuracy 0.972051 \n",
      "training loss:  0.09411710031424149\n",
      "valid loss 0.094903 and accuracy 0.969738 \n",
      "training loss:  0.09726257695861393\n",
      "valid loss 0.091104 and accuracy 0.971762 \n",
      "training loss:  0.08877885792224674\n",
      "valid loss 0.093543 and accuracy 0.967618 \n",
      "training loss:  0.09299237192199078\n",
      "valid loss 0.093424 and accuracy 0.969834 \n",
      "training loss:  0.09224872137693797\n",
      "valid loss 0.092972 and accuracy 0.971087 \n",
      "training loss:  0.09418294926697815\n",
      "valid loss 0.098007 and accuracy 0.964726 \n",
      "training loss:  0.09273889353061777\n",
      "valid loss 0.091239 and accuracy 0.971473 \n",
      "training loss:  0.08922742038807645\n",
      "valid loss 0.097422 and accuracy 0.965594 \n",
      "training loss:  0.09088477453183672\n",
      "valid loss 0.093126 and accuracy 0.970702 \n",
      "training loss:  0.08937860431699901\n",
      "valid loss 0.088282 and accuracy 0.973400 \n",
      "training loss:  0.0885083750198101\n",
      "valid loss 0.088377 and accuracy 0.970509 \n",
      "training loss:  0.08884263697270518\n",
      "valid loss 0.090509 and accuracy 0.971858 \n",
      "training loss:  0.09064540235967095\n",
      "valid loss 0.091682 and accuracy 0.970509 \n",
      "training loss:  0.08933493152989588\n",
      "valid loss 0.094095 and accuracy 0.967039 \n",
      "training loss:  0.09550146306744886\n",
      "valid loss 0.106693 and accuracy 0.960968 \n",
      "training loss:  0.09095616111013563\n",
      "valid loss 0.091401 and accuracy 0.971280 \n",
      "training loss:  0.09540092709467494\n",
      "valid loss 0.093093 and accuracy 0.970316 \n",
      "training loss:  0.08461702944832729\n",
      "valid loss 0.088186 and accuracy 0.972918 \n",
      "training loss:  0.08476947576314782\n",
      "valid loss 0.087898 and accuracy 0.973207 \n",
      "training loss:  0.08998758195156462\n",
      "valid loss 0.087808 and accuracy 0.971955 \n",
      "training loss:  0.09337485147306483\n",
      "valid loss 0.099456 and accuracy 0.963859 \n",
      "training loss:  0.09068328603210822\n",
      "valid loss 0.092999 and accuracy 0.968485 \n",
      "training loss:  0.0859150042940825\n",
      "valid loss 0.092859 and accuracy 0.967136 \n",
      "training loss:  0.09263455532417342\n",
      "valid loss 0.097215 and accuracy 0.966172 \n",
      "training loss:  0.08992431006872607\n",
      "valid loss 0.088949 and accuracy 0.972147 \n",
      "training loss:  0.08997994957532382\n",
      "valid loss 0.094623 and accuracy 0.966943 \n",
      "training loss:  0.0855459272393665\n",
      "valid loss 0.091030 and accuracy 0.967907 \n",
      "training loss:  0.08377491746046097\n",
      "valid loss 0.086643 and accuracy 0.972822 \n",
      "training loss:  0.09151483382303353\n",
      "valid loss 0.090817 and accuracy 0.971184 \n",
      "training loss:  0.0865148218854602\n",
      "valid loss 0.085788 and accuracy 0.972822 \n",
      "training loss:  0.08431717853867605\n",
      "valid loss 0.087202 and accuracy 0.970027 \n",
      "training loss:  0.08683261824113127\n",
      "valid loss 0.093127 and accuracy 0.966654 \n",
      "training loss:  0.08510341879869247\n",
      "valid loss 0.094530 and accuracy 0.967810 \n",
      "training loss:  0.08696955560859827\n",
      "valid loss 0.088926 and accuracy 0.971955 \n",
      "training loss:  0.08333223762442478\n",
      "valid loss 0.087420 and accuracy 0.972436 \n",
      "training loss:  0.08561518777011644\n",
      "valid loss 0.087226 and accuracy 0.972726 \n",
      "training loss:  0.08560647087643183\n",
      "valid loss 0.087051 and accuracy 0.972726 \n",
      "training loss:  0.08757827250238924\n",
      "valid loss 0.089401 and accuracy 0.971569 \n",
      "training loss:  0.08233487213822051\n",
      "valid loss 0.084315 and accuracy 0.974171 \n",
      "training loss:  0.08334938156303395\n",
      "valid loss 0.087205 and accuracy 0.972147 \n",
      "training loss:  0.08493034787825693\n",
      "valid loss 0.083358 and accuracy 0.974557 \n",
      "training loss:  0.08316663824519178\n",
      "valid loss 0.084731 and accuracy 0.973593 \n",
      "training loss:  0.08112227315574352\n",
      "valid loss 0.082577 and accuracy 0.973689 \n",
      "training loss:  0.08198032240332237\n",
      "valid loss 0.085962 and accuracy 0.973882 \n",
      "training loss:  0.079321953871208\n",
      "valid loss 0.081126 and accuracy 0.973689 \n",
      "training loss:  0.08005689973133491\n",
      "valid loss 0.081886 and accuracy 0.973207 \n",
      "training loss:  0.08128020262268112\n",
      "valid loss 0.083539 and accuracy 0.972244 \n",
      "training loss:  0.08077243770185245\n",
      "valid loss 0.083482 and accuracy 0.973593 \n",
      "training loss:  0.08620216709765392\n",
      "valid loss 0.081885 and accuracy 0.975039 \n",
      "training loss:  0.0831928789352177\n",
      "valid loss 0.082550 and accuracy 0.974268 \n",
      "training loss:  0.0833797120493489\n",
      "valid loss 0.088227 and accuracy 0.968292 \n",
      "training loss:  0.08669211549015922\n",
      "valid loss 0.093122 and accuracy 0.968099 \n",
      "training loss:  0.08096149707334922\n",
      "valid loss 0.081344 and accuracy 0.976484 \n",
      "training loss:  0.0795023514638653\n",
      "valid loss 0.082954 and accuracy 0.975039 \n",
      "training loss:  0.08092735009852696\n",
      "valid loss 0.082966 and accuracy 0.972340 \n",
      "training loss:  0.08251243963916413\n",
      "valid loss 0.079704 and accuracy 0.976099 \n",
      "training loss:  0.08113557670908067\n",
      "valid loss 0.084872 and accuracy 0.972533 \n",
      "training loss:  0.08455161148804115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss 0.084248 and accuracy 0.970894 \n",
      "training loss:  0.0843842652492762\n",
      "valid loss 0.082748 and accuracy 0.975135 \n",
      "training loss:  0.07942442883049819\n",
      "valid loss 0.079854 and accuracy 0.976484 \n",
      "training loss:  0.08093319668312067\n",
      "valid loss 0.080343 and accuracy 0.976291 \n",
      "training loss:  0.07635019181243292\n",
      "valid loss 0.077724 and accuracy 0.975810 \n",
      "training loss:  0.07873663436239789\n",
      "valid loss 0.077049 and accuracy 0.976002 \n",
      "training loss:  0.08053684864939244\n",
      "valid loss 0.088339 and accuracy 0.967618 \n",
      "training loss:  0.07882888694413828\n",
      "valid loss 0.078676 and accuracy 0.977833 \n",
      "training loss:  0.08621348915233844\n",
      "valid loss 0.078200 and accuracy 0.977062 \n",
      "training loss:  0.07670866028927094\n",
      "valid loss 0.076564 and accuracy 0.977062 \n",
      "training loss:  0.0750380117219663\n",
      "valid loss 0.074705 and accuracy 0.979279 \n",
      "training loss:  0.0811005455921284\n",
      "valid loss 0.078679 and accuracy 0.974460 \n",
      "training loss:  0.07580643216663847\n",
      "valid loss 0.077220 and accuracy 0.975713 \n",
      "training loss:  0.07502834771938932\n",
      "valid loss 0.073703 and accuracy 0.976966 \n",
      "training loss:  0.07339727055091812\n",
      "valid loss 0.084824 and accuracy 0.969256 \n",
      "training loss:  0.0823629928502868\n",
      "valid loss 0.095871 and accuracy 0.964630 \n",
      "training loss:  0.0792302704980672\n",
      "valid loss 0.076150 and accuracy 0.974846 \n",
      "training loss:  0.07624744926748572\n",
      "valid loss 0.072627 and accuracy 0.979375 \n",
      "training loss:  0.07710162574217284\n",
      "valid loss 0.072621 and accuracy 0.978219 \n",
      "training loss:  0.07411704818125364\n",
      "valid loss 0.075577 and accuracy 0.978990 \n",
      "training loss:  0.07831258618045704\n",
      "valid loss 0.076728 and accuracy 0.974846 \n",
      "training loss:  0.07836652981943432\n",
      "valid loss 0.073991 and accuracy 0.978123 \n",
      "training loss:  0.0770627943699181\n",
      "valid loss 0.071049 and accuracy 0.979761 \n",
      "training loss:  0.08074290897409134\n",
      "valid loss 0.072686 and accuracy 0.977641 \n",
      "training loss:  0.08150821968775196\n",
      "valid loss 0.074256 and accuracy 0.978894 \n",
      "training loss:  0.07694968431129293\n",
      "valid loss 0.071294 and accuracy 0.980918 \n",
      "training loss:  0.07469662165629966\n",
      "valid loss 0.068832 and accuracy 0.979279 \n",
      "training loss:  0.07802932556307403\n",
      "valid loss 0.071232 and accuracy 0.980725 \n",
      "training loss:  0.07908581942073652\n",
      "valid loss 0.072600 and accuracy 0.977641 \n",
      "training loss:  0.07271798717083319\n",
      "valid loss 0.073932 and accuracy 0.976677 \n",
      "training loss:  0.07330066089992955\n",
      "valid loss 0.078363 and accuracy 0.974749 \n",
      "training loss:  0.07122923385644897\n",
      "valid loss 0.069208 and accuracy 0.981496 \n",
      "training loss:  0.07467545235932109\n",
      "valid loss 0.070252 and accuracy 0.981014 \n",
      "training loss:  0.08409167786288246\n",
      "valid loss 0.086902 and accuracy 0.968099 \n",
      "training loss:  0.08267200354090291\n",
      "valid loss 0.073286 and accuracy 0.978990 \n",
      "training loss:  0.07584737678060292\n",
      "valid loss 0.071120 and accuracy 0.980532 \n",
      "training loss:  0.07409119436865758\n",
      "valid loss 0.069594 and accuracy 0.982363 \n",
      "training loss:  0.07572658390781774\n",
      "valid loss 0.073893 and accuracy 0.977544 \n",
      "training loss:  0.07717841350512243\n",
      "valid loss 0.074157 and accuracy 0.979665 \n",
      "training loss:  0.08191542541737479\n",
      "valid loss 0.076810 and accuracy 0.977448 \n",
      "training loss:  0.07747489488600542\n",
      "valid loss 0.073525 and accuracy 0.978701 \n",
      "training loss:  0.07189584699732061\n",
      "valid loss 0.070082 and accuracy 0.981689 \n",
      "training loss:  0.0772313827073838\n",
      "valid loss 0.079931 and accuracy 0.974460 \n",
      "training loss:  0.0778893852361428\n",
      "valid loss 0.072684 and accuracy 0.980628 \n",
      "training loss:  0.07955966712826935\n",
      "valid loss 0.076201 and accuracy 0.975713 \n",
      "training loss:  0.08002744996963317\n",
      "valid loss 0.078405 and accuracy 0.974557 \n",
      "training loss:  0.07481939686365027\n",
      "valid loss 0.069534 and accuracy 0.981881 \n",
      "training loss:  0.07573614535657523\n",
      "valid loss 0.072758 and accuracy 0.979568 \n",
      "training loss:  0.08736253445457128\n",
      "valid loss 0.074639 and accuracy 0.980243 \n",
      "training loss:  0.08122303385363851\n",
      "valid loss 0.071539 and accuracy 0.981110 \n",
      "training loss:  0.07293435561138907\n",
      "valid loss 0.068382 and accuracy 0.982652 \n",
      "training loss:  0.07781734120319872\n",
      "valid loss 0.072926 and accuracy 0.981303 \n",
      "training loss:  0.06949796778274858\n",
      "valid loss 0.068022 and accuracy 0.982941 \n",
      "training loss:  0.07678071240311574\n",
      "valid loss 0.074887 and accuracy 0.978508 \n",
      "training loss:  0.0744421584205425\n",
      "valid loss 0.071965 and accuracy 0.979568 \n",
      "training loss:  0.06833320213709536\n",
      "valid loss 0.064618 and accuracy 0.983712 \n",
      "training loss:  0.07545739033130802\n",
      "valid loss 0.065627 and accuracy 0.983809 \n",
      "training loss:  0.07309777392548437\n",
      "valid loss 0.068580 and accuracy 0.982074 \n",
      "training loss:  0.07827241468364564\n",
      "valid loss 0.067321 and accuracy 0.981785 \n",
      "training loss:  0.07741259219049816\n",
      "valid loss 0.067841 and accuracy 0.982556 \n",
      "training loss:  0.07481743356549063\n",
      "valid loss 0.070720 and accuracy 0.981785 \n",
      "training loss:  0.07987625426160368\n",
      "valid loss 0.071515 and accuracy 0.980628 \n",
      "training loss:  0.07717222344900942\n",
      "valid loss 0.073821 and accuracy 0.980339 \n",
      "training loss:  0.07527593339461604\n",
      "valid loss 0.071306 and accuracy 0.981592 \n",
      "training loss:  0.07646907146180495\n",
      "valid loss 0.066818 and accuracy 0.984676 \n",
      "training loss:  0.07388748397579156\n",
      "valid loss 0.067043 and accuracy 0.983809 \n",
      "training loss:  0.07221755506907838\n",
      "valid loss 0.072739 and accuracy 0.980050 \n",
      "training loss:  0.07413459935589335\n",
      "valid loss 0.075806 and accuracy 0.974460 \n",
      "training loss:  0.07944700277736072\n",
      "valid loss 0.071806 and accuracy 0.979857 \n",
      "training loss:  0.07548225448840475\n",
      "valid loss 0.068683 and accuracy 0.982363 \n",
      "training loss:  0.07240620958671141\n",
      "valid loss 0.065666 and accuracy 0.984965 \n",
      "training loss:  0.08246109672924855\n",
      "valid loss 0.072018 and accuracy 0.981110 \n",
      "training loss:  0.0744430232258798\n",
      "valid loss 0.068757 and accuracy 0.981881 \n",
      "training loss:  0.07821203069730619\n",
      "valid loss 0.071853 and accuracy 0.978315 \n",
      "training loss:  0.075962879881509\n",
      "valid loss 0.067095 and accuracy 0.984291 \n",
      "training loss:  0.07442130556272224\n",
      "valid loss 0.070058 and accuracy 0.981110 \n",
      "training loss:  0.07732862135381866\n",
      "valid loss 0.076459 and accuracy 0.977641 \n",
      "training loss:  0.07977234319059248\n",
      "valid loss 0.079057 and accuracy 0.974942 \n",
      "training loss:  0.07548426742919691\n",
      "valid loss 0.070856 and accuracy 0.982170 \n",
      "training loss:  0.07273591709891378\n",
      "valid loss 0.067701 and accuracy 0.981978 \n",
      "training loss:  0.07364201856566555\n",
      "valid loss 0.066050 and accuracy 0.983809 \n",
      "training loss:  0.07702914286855428\n",
      "valid loss 0.071956 and accuracy 0.980628 \n",
      "training loss:  0.08058775702376911\n",
      "valid loss 0.081349 and accuracy 0.974364 \n",
      "training loss:  0.07705264656711554\n",
      "valid loss 0.070640 and accuracy 0.984194 \n",
      "training loss:  0.08056877251968074\n",
      "valid loss 0.080222 and accuracy 0.978026 \n",
      "training loss:  0.09885146059415945\n",
      "valid loss 0.072835 and accuracy 0.981110 \n",
      "training loss:  0.08694627270673728\n",
      "valid loss 0.081280 and accuracy 0.978701 \n",
      "training loss:  0.07647804418173713\n",
      "valid loss 0.072871 and accuracy 0.982652 \n",
      "training loss:  0.07925739648498925\n",
      "valid loss 0.073211 and accuracy 0.982556 \n",
      "training loss:  0.07781245718388194\n",
      "valid loss 0.071072 and accuracy 0.983809 \n",
      "training loss:  0.07574312889705961\n",
      "valid loss 0.069282 and accuracy 0.983616 \n",
      "training loss:  0.07415275523649567\n",
      "valid loss 0.070023 and accuracy 0.985158 \n",
      "training loss:  0.07955591684430148\n",
      "valid loss 0.073694 and accuracy 0.981110 \n",
      "training loss:  0.07941979474295115\n",
      "valid loss 0.068683 and accuracy 0.984194 \n",
      "training loss:  0.07792423070412713\n",
      "valid loss 0.073400 and accuracy 0.982460 \n",
      "training loss:  0.07461336566540318\n",
      "valid loss 0.073419 and accuracy 0.979279 \n",
      "training loss:  0.07269751950630626\n",
      "valid loss 0.072408 and accuracy 0.983809 \n",
      "training loss:  0.08091373682288011\n",
      "valid loss 0.077431 and accuracy 0.980050 \n",
      "training loss:  0.08242697596195432\n",
      "valid loss 0.073511 and accuracy 0.982170 \n",
      "training loss:  0.07743684544184314\n",
      "valid loss 0.069916 and accuracy 0.983038 \n",
      "training loss:  0.08027262246311302\n",
      "valid loss 0.075855 and accuracy 0.979375 \n",
      "training loss:  0.07469941411347122\n",
      "valid loss 0.071635 and accuracy 0.982652 \n",
      "training loss:  0.07628101346459085\n",
      "valid loss 0.079319 and accuracy 0.978315 \n",
      "training loss:  0.07553752235605357\n",
      "valid loss 0.077377 and accuracy 0.980725 \n",
      "training loss:  0.07734977497035356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss 0.069370 and accuracy 0.984194 \n",
      "training loss:  0.07775101000018374\n",
      "valid loss 0.071933 and accuracy 0.984194 \n",
      "training loss:  0.08104140705183414\n",
      "valid loss 0.079431 and accuracy 0.981399 \n",
      "training loss:  0.08126385461708921\n",
      "valid loss 0.072851 and accuracy 0.982267 \n",
      "training loss:  0.08087886087054735\n",
      "valid loss 0.076513 and accuracy 0.982556 \n",
      "training loss:  0.07417003357732485\n",
      "valid loss 0.074007 and accuracy 0.983327 \n",
      "training loss:  0.08066914440609727\n",
      "valid loss 0.072634 and accuracy 0.983809 \n",
      "training loss:  0.07721472069404414\n",
      "valid loss 0.071022 and accuracy 0.982363 \n",
      "training loss:  0.08512987852618542\n",
      "valid loss 0.076068 and accuracy 0.982267 \n",
      "training loss:  0.07656045797879894\n",
      "valid loss 0.073260 and accuracy 0.984291 \n",
      "training loss:  0.08109442021399835\n",
      "valid loss 0.081311 and accuracy 0.980918 \n",
      "training loss:  0.08265006714312745\n",
      "valid loss 0.072281 and accuracy 0.985158 \n",
      "training loss:  0.07577121954088721\n",
      "valid loss 0.074633 and accuracy 0.983327 \n",
      "training loss:  0.07863952368694824\n",
      "valid loss 0.074614 and accuracy 0.983809 \n",
      "training loss:  0.08663525335091829\n",
      "valid loss 0.076593 and accuracy 0.983809 \n",
      "training loss:  0.08201521408041818\n",
      "valid loss 0.076280 and accuracy 0.983520 \n",
      "training loss:  0.08133709097068119\n",
      "valid loss 0.074992 and accuracy 0.983134 \n",
      "training loss:  0.08335841790343938\n",
      "valid loss 0.083028 and accuracy 0.977448 \n",
      "training loss:  0.08550858860915454\n",
      "valid loss 0.077406 and accuracy 0.983520 \n",
      "training loss:  0.07946538964812123\n",
      "valid loss 0.075085 and accuracy 0.983134 \n",
      "training loss:  0.0787416765735942\n",
      "valid loss 0.074702 and accuracy 0.983134 \n",
      "training loss:  0.08115447876680709\n",
      "valid loss 0.080545 and accuracy 0.978219 \n",
      "training loss:  0.08556744794349598\n",
      "valid loss 0.076596 and accuracy 0.982460 \n",
      "training loss:  0.08465364801168677\n",
      "valid loss 0.076160 and accuracy 0.984483 \n",
      "training loss:  0.08444548480739597\n",
      "valid loss 0.080996 and accuracy 0.982363 \n",
      "training loss:  0.08341356049742986\n",
      "valid loss 0.077866 and accuracy 0.983423 \n",
      "training loss:  0.0802107923427167\n",
      "valid loss 0.079650 and accuracy 0.980436 \n",
      "training loss:  0.08181841492342658\n",
      "valid loss 0.077186 and accuracy 0.980918 \n",
      "training loss:  0.08462327915709773\n",
      "valid loss 0.079051 and accuracy 0.982460 \n",
      "training loss:  0.08501813606274734\n",
      "valid loss 0.081701 and accuracy 0.979086 \n",
      "training loss:  0.085655424288386\n",
      "valid loss 0.080000 and accuracy 0.980532 \n",
      "training loss:  0.08157080787887104\n",
      "valid loss 0.078839 and accuracy 0.984002 \n",
      "training loss:  0.08227541343717093\n",
      "valid loss 0.079344 and accuracy 0.982267 \n",
      "training loss:  0.08508370251635487\n",
      "valid loss 0.081670 and accuracy 0.982749 \n",
      "training loss:  0.08547842496930962\n",
      "valid loss 0.082424 and accuracy 0.982170 \n",
      "training loss:  0.08484738577657083\n",
      "valid loss 0.077170 and accuracy 0.985062 \n",
      "training loss:  0.08539850684098274\n",
      "valid loss 0.081946 and accuracy 0.982556 \n",
      "training loss:  0.08757340771891546\n",
      "valid loss 0.083019 and accuracy 0.980725 \n",
      "training loss:  0.08675053395375225\n",
      "valid loss 0.082919 and accuracy 0.978990 \n",
      "training loss:  0.08285717502906081\n",
      "valid loss 0.078334 and accuracy 0.984676 \n",
      "training loss:  0.0897722807955994\n",
      "valid loss 0.082945 and accuracy 0.981689 \n",
      "training loss:  0.08781017336726031\n",
      "valid loss 0.083497 and accuracy 0.981689 \n",
      "training loss:  0.08498688251449821\n",
      "valid loss 0.081710 and accuracy 0.980725 \n",
      "training loss:  0.08816712265692636\n",
      "valid loss 0.093939 and accuracy 0.981399 \n",
      "training loss:  0.0913401024966388\n",
      "valid loss 0.081014 and accuracy 0.981014 \n",
      "training loss:  0.0912732407032104\n",
      "valid loss 0.085165 and accuracy 0.983520 \n",
      "training loss:  0.08889008591644668\n",
      "valid loss 0.081266 and accuracy 0.982941 \n",
      "training loss:  0.08672485496634612\n",
      "valid loss 0.084787 and accuracy 0.982460 \n",
      "training loss:  0.08503000022349812\n",
      "valid loss 0.080762 and accuracy 0.985544 \n",
      "training loss:  0.08885710493902142\n",
      "valid loss 0.082956 and accuracy 0.983038 \n",
      "training loss:  0.08661101098859933\n",
      "valid loss 0.083477 and accuracy 0.981110 \n",
      "training loss:  0.08293289667819008\n",
      "valid loss 0.082096 and accuracy 0.984580 \n",
      "training loss:  0.08895996539199187\n",
      "valid loss 0.086871 and accuracy 0.983809 \n",
      "training loss:  0.0864765403763947\n",
      "valid loss 0.080838 and accuracy 0.986315 \n",
      "training loss:  0.09192653950020642\n",
      "valid loss 0.086234 and accuracy 0.981110 \n",
      "training loss:  0.09109625927439133\n",
      "valid loss 0.084871 and accuracy 0.982845 \n",
      "training loss:  0.08623465376377341\n",
      "valid loss 0.084926 and accuracy 0.982556 \n",
      "training loss:  0.08716471777873093\n",
      "valid loss 0.082711 and accuracy 0.984194 \n",
      "training loss:  0.08953892274390997\n",
      "valid loss 0.084571 and accuracy 0.981785 \n",
      "training loss:  0.09145226610993126\n",
      "valid loss 0.091411 and accuracy 0.978604 \n",
      "training loss:  0.08999303113041299\n",
      "valid loss 0.088881 and accuracy 0.982170 \n",
      "training loss:  0.09452110323599451\n",
      "valid loss 0.089145 and accuracy 0.982749 \n",
      "training loss:  0.08839912320347472\n",
      "valid loss 0.083562 and accuracy 0.984869 \n",
      "training loss:  0.0904297227260299\n",
      "valid loss 0.085825 and accuracy 0.984676 \n",
      "training loss:  0.09117825495993863\n",
      "valid loss 0.088532 and accuracy 0.983038 \n",
      "training loss:  0.09028538604318032\n",
      "valid loss 0.087756 and accuracy 0.984002 \n",
      "training loss:  0.0927802032402668\n",
      "valid loss 0.090427 and accuracy 0.982460 \n",
      "training loss:  0.09391669154177024\n",
      "valid loss 0.085824 and accuracy 0.984387 \n",
      "training loss:  0.0877960446338069\n",
      "valid loss 0.086469 and accuracy 0.984676 \n",
      "training loss:  0.09630552711801188\n",
      "valid loss 0.089351 and accuracy 0.984676 \n",
      "training loss:  0.09189311051291893\n",
      "valid loss 0.088434 and accuracy 0.983134 \n",
      "training loss:  0.09193708767076636\n",
      "valid loss 0.091427 and accuracy 0.979761 \n",
      "training loss:  0.09360847379876854\n",
      "valid loss 0.096154 and accuracy 0.976773 \n",
      "training loss:  0.09388902735873633\n",
      "valid loss 0.088313 and accuracy 0.984194 \n",
      "training loss:  0.09290678748431304\n",
      "valid loss 0.087868 and accuracy 0.983809 \n",
      "training loss:  0.09259526662513742\n",
      "valid loss 0.089961 and accuracy 0.985158 \n",
      "training loss:  0.09521753181655991\n",
      "valid loss 0.094521 and accuracy 0.982170 \n",
      "training loss:  0.09171183062107927\n",
      "valid loss 0.086551 and accuracy 0.985351 \n",
      "training loss:  0.09457442221543841\n",
      "valid loss 0.092848 and accuracy 0.982363 \n",
      "training loss:  0.09415723749554007\n",
      "valid loss 0.090552 and accuracy 0.983905 \n",
      "training loss:  0.09696795879780104\n",
      "valid loss 0.092130 and accuracy 0.985062 \n",
      "training loss:  0.09772630181146313\n",
      "valid loss 0.093932 and accuracy 0.984773 \n",
      "training loss:  0.09619366301264262\n",
      "valid loss 0.091200 and accuracy 0.981978 \n",
      "training loss:  0.09641069417815687\n",
      "valid loss 0.091647 and accuracy 0.981785 \n",
      "training loss:  0.10458684948033556\n",
      "valid loss 0.096859 and accuracy 0.981592 \n",
      "training loss:  0.09462849831027616\n",
      "valid loss 0.089225 and accuracy 0.985447 \n",
      "training loss:  0.09502242913288701\n",
      "valid loss 0.089475 and accuracy 0.985447 \n",
      "training loss:  0.09783740325905357\n",
      "valid loss 0.093122 and accuracy 0.983231 \n",
      "training loss:  0.09768989323210575\n",
      "valid loss 0.122999 and accuracy 0.969256 \n",
      "training loss:  0.09946624530874493\n",
      "valid loss 0.093297 and accuracy 0.984291 \n",
      "training loss:  0.0981207038525194\n",
      "valid loss 0.091291 and accuracy 0.984098 \n",
      "training loss:  0.0966632617742158\n",
      "valid loss 0.092685 and accuracy 0.984194 \n",
      "training loss:  0.1016602482663939\n",
      "valid loss 0.091854 and accuracy 0.984098 \n",
      "training loss:  0.09798759278168524\n",
      "valid loss 0.092618 and accuracy 0.984869 \n",
      "training loss:  0.09842759044810155\n",
      "valid loss 0.098464 and accuracy 0.981785 \n",
      "training loss:  0.09614460322924663\n",
      "valid loss 0.094760 and accuracy 0.984580 \n",
      "training loss:  0.10169488516367199\n",
      "valid loss 0.090229 and accuracy 0.984387 \n",
      "training loss:  0.09989966197285287\n",
      "valid loss 0.094187 and accuracy 0.984291 \n",
      "training loss:  0.09962627672087414\n",
      "valid loss 0.093085 and accuracy 0.984773 \n",
      "training loss:  0.1004736261603577\n",
      "valid loss 0.094932 and accuracy 0.983809 \n",
      "training loss:  0.10045324557972364\n",
      "valid loss 0.094136 and accuracy 0.984194 \n",
      "training loss:  0.09526484105083538\n",
      "valid loss 0.093732 and accuracy 0.981014 \n",
      "training loss:  0.09783010898306917\n",
      "valid loss 0.106658 and accuracy 0.981785 \n",
      "training loss:  0.09837706149195648\n",
      "valid loss 0.091002 and accuracy 0.986122 \n",
      "training loss:  0.09786115867447082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss 0.095883 and accuracy 0.982941 \n",
      "training loss:  0.09904812365740794\n",
      "valid loss 0.094521 and accuracy 0.978508 \n",
      "training loss:  0.09549235061096657\n",
      "valid loss 0.091589 and accuracy 0.985351 \n",
      "training loss:  0.10278566090804965\n",
      "valid loss 0.094373 and accuracy 0.984676 \n",
      "training loss:  0.09715449689627087\n",
      "valid loss 0.097646 and accuracy 0.983905 \n",
      "training loss:  0.10273534695015234\n",
      "valid loss 0.095441 and accuracy 0.979568 \n",
      "training loss:  0.0986249762496764\n",
      "valid loss 0.093456 and accuracy 0.984387 \n",
      "training loss:  0.10625877316058746\n",
      "valid loss 0.096029 and accuracy 0.984098 \n",
      "training loss:  0.09941371031701368\n",
      "valid loss 0.094527 and accuracy 0.984291 \n",
      "training loss:  0.09474632669651421\n",
      "valid loss 0.091600 and accuracy 0.983423 \n",
      "training loss:  0.09733984770557381\n",
      "valid loss 0.097021 and accuracy 0.981014 \n",
      "training loss:  0.10276726416637783\n",
      "valid loss 0.094909 and accuracy 0.982749 \n",
      "training loss:  0.09861465548432906\n",
      "valid loss 0.095262 and accuracy 0.984483 \n",
      "training loss:  0.09988294457875177\n",
      "valid loss 0.093106 and accuracy 0.983327 \n",
      "training loss:  0.10194584107800816\n",
      "valid loss 0.098540 and accuracy 0.976870 \n",
      "training loss:  0.1033741756264177\n",
      "valid loss 0.103671 and accuracy 0.975906 \n",
      "training loss:  0.09922749221679476\n",
      "valid loss 0.096044 and accuracy 0.982267 \n",
      "training loss:  0.09967570561085878\n",
      "valid loss 0.094578 and accuracy 0.984002 \n",
      "training loss:  0.09932076891112313\n",
      "valid loss 0.098269 and accuracy 0.983520 \n",
      "training loss:  0.09423875594505472\n",
      "valid loss 0.092688 and accuracy 0.981303 \n",
      "training loss:  0.09889720220017811\n",
      "valid loss 0.097044 and accuracy 0.982652 \n",
      "training loss:  0.09635627501682974\n",
      "valid loss 0.094714 and accuracy 0.985833 \n",
      "training loss:  0.10735226455541612\n",
      "valid loss 0.094611 and accuracy 0.984580 \n",
      "training loss:  0.10037220138699426\n",
      "valid loss 0.095205 and accuracy 0.984194 \n",
      "training loss:  0.09725971374431747\n",
      "valid loss 0.092513 and accuracy 0.984773 \n",
      "training loss:  0.10323447472386933\n",
      "valid loss 0.094533 and accuracy 0.983809 \n",
      "training loss:  0.09813537350157121\n",
      "valid loss 0.101057 and accuracy 0.982170 \n",
      "training loss:  0.10222203385205515\n",
      "valid loss 0.099132 and accuracy 0.983712 \n",
      "training loss:  0.10379970073207513\n",
      "valid loss 0.099935 and accuracy 0.983231 \n",
      "training loss:  0.09567114605918596\n",
      "valid loss 0.094815 and accuracy 0.983616 \n",
      "training loss:  0.09670378199316221\n",
      "valid loss 0.094429 and accuracy 0.983327 \n",
      "training loss:  0.1027631975158862\n",
      "valid loss 0.096894 and accuracy 0.982363 \n",
      "training loss:  0.1026240993938695\n",
      "valid loss 0.095398 and accuracy 0.982652 \n",
      "training loss:  0.09900969788136658\n",
      "valid loss 0.099803 and accuracy 0.983712 \n",
      "training loss:  0.0988321465939303\n",
      "valid loss 0.096643 and accuracy 0.984483 \n",
      "training loss:  0.10533954723635337\n",
      "valid loss 0.094657 and accuracy 0.982363 \n",
      "training loss:  0.09839399593530786\n",
      "valid loss 0.097103 and accuracy 0.986411 \n",
      "training loss:  0.10167421081693694\n",
      "valid loss 0.095125 and accuracy 0.985062 \n",
      "training loss:  0.1027024950824742\n",
      "valid loss 0.095767 and accuracy 0.983905 \n",
      "training loss:  0.09931253198896192\n",
      "valid loss 0.097700 and accuracy 0.981689 \n",
      "training loss:  0.09958787523185426\n",
      "valid loss 0.094829 and accuracy 0.984291 \n",
      "training loss:  0.10676268576561262\n",
      "valid loss 0.098331 and accuracy 0.983616 \n",
      "training loss:  0.10045052841995146\n",
      "valid loss 0.100670 and accuracy 0.978412 \n",
      "training loss:  0.10501981074063099\n",
      "valid loss 0.095868 and accuracy 0.984580 \n",
      "training loss:  0.10068762617804952\n",
      "valid loss 0.097826 and accuracy 0.984002 \n",
      "training loss:  0.09711596915610406\n",
      "valid loss 0.095659 and accuracy 0.984773 \n",
      "training loss:  0.10428338504771247\n",
      "valid loss 0.098000 and accuracy 0.983231 \n",
      "training loss:  0.09740276974228858\n",
      "valid loss 0.094174 and accuracy 0.985929 \n",
      "training loss:  0.10215620016734567\n",
      "valid loss 0.097452 and accuracy 0.981689 \n",
      "training loss:  0.09841055825835257\n",
      "valid loss 0.093952 and accuracy 0.984291 \n",
      "training loss:  0.09754929584940379\n",
      "valid loss 0.093914 and accuracy 0.984483 \n",
      "training loss:  0.09752608501332057\n",
      "valid loss 0.100199 and accuracy 0.977159 \n",
      "training loss:  0.09822213308736731\n",
      "valid loss 0.094981 and accuracy 0.982267 \n",
      "training loss:  0.10886853589120482\n",
      "valid loss 0.098437 and accuracy 0.977641 \n",
      "training loss:  0.10502562739856182\n",
      "valid loss 0.098918 and accuracy 0.982267 \n",
      "training loss:  0.10505081697441612\n",
      "valid loss 0.098693 and accuracy 0.981881 \n",
      "training loss:  0.10159315867134064\n",
      "valid loss 0.097123 and accuracy 0.984869 \n",
      "training loss:  0.09987771797676553\n",
      "valid loss 0.099054 and accuracy 0.983327 \n",
      "training loss:  0.09694275118642033\n",
      "valid loss 0.094237 and accuracy 0.983520 \n",
      "training loss:  0.11072803497984168\n",
      "valid loss 0.101775 and accuracy 0.982460 \n",
      "training loss:  0.10067064398795031\n",
      "valid loss 0.104244 and accuracy 0.980436 \n",
      "training loss:  0.10189489561569509\n",
      "valid loss 0.094526 and accuracy 0.980918 \n",
      "training loss:  0.10304124332023865\n",
      "valid loss 0.102249 and accuracy 0.973882 \n",
      "training loss:  0.09772905135146845\n",
      "valid loss 0.093577 and accuracy 0.984387 \n",
      "training loss:  0.09966969518366431\n",
      "valid loss 0.094136 and accuracy 0.985062 \n",
      "training loss:  0.10055177757530046\n",
      "valid loss 0.101561 and accuracy 0.974364 \n",
      "training loss:  0.10272450575953435\n",
      "valid loss 0.104510 and accuracy 0.981592 \n",
      "training loss:  0.099388790235012\n",
      "valid loss 0.093472 and accuracy 0.983905 \n",
      "training loss:  0.10371883335398352\n",
      "valid loss 0.098042 and accuracy 0.978315 \n",
      "training loss:  0.10128490443132764\n",
      "valid loss 0.099069 and accuracy 0.984194 \n",
      "training loss:  0.0984541683729248\n",
      "valid loss 0.098291 and accuracy 0.983905 \n",
      "training loss:  0.10448075448066213\n",
      "valid loss 0.094583 and accuracy 0.982170 \n",
      "training loss:  0.09991112472803405\n",
      "valid loss 0.097444 and accuracy 0.984483 \n",
      "training loss:  0.09987923066622362\n",
      "valid loss 0.095040 and accuracy 0.984483 \n",
      "training loss:  0.09871747825597582\n",
      "valid loss 0.092910 and accuracy 0.984580 \n",
      "training loss:  0.10188320043982454\n",
      "valid loss 0.091296 and accuracy 0.982460 \n",
      "training loss:  0.10284413321476776\n",
      "valid loss 0.097164 and accuracy 0.980532 \n",
      "training loss:  0.09811326267748027\n",
      "valid loss 0.100828 and accuracy 0.981978 \n",
      "training loss:  0.09876687062925167\n",
      "valid loss 0.096995 and accuracy 0.985158 \n",
      "training loss:  0.10393705343124651\n",
      "valid loss 0.098322 and accuracy 0.983712 \n",
      "training loss:  0.10013115811804943\n",
      "valid loss 0.099192 and accuracy 0.983327 \n",
      "training loss:  0.10064862653494905\n",
      "valid loss 0.096595 and accuracy 0.983809 \n",
      "training loss:  0.10185947298008877\n",
      "valid loss 0.098690 and accuracy 0.983231 \n",
      "training loss:  0.09790737920804522\n",
      "valid loss 0.098706 and accuracy 0.983134 \n",
      "training loss:  0.10225586415627344\n",
      "valid loss 0.099720 and accuracy 0.986604 \n",
      "training loss:  0.10210451178415202\n",
      "valid loss 0.095854 and accuracy 0.982749 \n",
      "training loss:  0.09981038789489532\n",
      "valid loss 0.091571 and accuracy 0.983423 \n",
      "training loss:  0.10228399061016191\n",
      "valid loss 0.098726 and accuracy 0.976002 \n",
      "training loss:  0.10364882825514929\n",
      "valid loss 0.095093 and accuracy 0.982556 \n",
      "training loss:  0.10440676986324181\n",
      "valid loss 0.099199 and accuracy 0.986122 \n",
      "training loss:  0.10090898450830876\n",
      "valid loss 0.096955 and accuracy 0.984098 \n",
      "training loss:  0.09967593739630368\n",
      "valid loss 0.098442 and accuracy 0.985833 \n",
      "training loss:  0.10102015645080108\n",
      "valid loss 0.095860 and accuracy 0.982460 \n",
      "training loss:  0.10503045185385992\n",
      "valid loss 0.100319 and accuracy 0.981881 \n",
      "training loss:  0.10103157952043763\n",
      "valid loss 0.102539 and accuracy 0.981592 \n",
      "training loss:  0.10452538639829007\n",
      "valid loss 0.101451 and accuracy 0.981881 \n",
      "training loss:  0.10245942510787573\n",
      "valid loss 0.101514 and accuracy 0.982363 \n",
      "training loss:  0.10333877879700279\n",
      "valid loss 0.095197 and accuracy 0.985062 \n",
      "training loss:  0.09869467921097075\n",
      "valid loss 0.098043 and accuracy 0.985254 \n",
      "training loss:  0.10336283227996328\n",
      "valid loss 0.094420 and accuracy 0.985833 \n",
      "training loss:  0.1009401873330249\n",
      "valid loss 0.094119 and accuracy 0.983616 \n",
      "training loss:  0.10110007119361959\n",
      "valid loss 0.097022 and accuracy 0.985254 \n",
      "training loss:  0.10346287432799493\n",
      "valid loss 0.110432 and accuracy 0.982170 \n",
      "training loss:  0.10188892118076338\n",
      "valid loss 0.124138 and accuracy 0.978797 \n",
      "training loss:  0.10105620344555385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss 0.095376 and accuracy 0.985158 \n",
      "training loss:  0.10373026137507994\n",
      "valid loss 0.095984 and accuracy 0.984387 \n",
      "training loss:  0.09626447205522244\n",
      "valid loss 0.094802 and accuracy 0.981303 \n",
      "training loss:  0.09542104534888535\n",
      "valid loss 0.094440 and accuracy 0.985640 \n",
      "training loss:  0.10120761923378695\n",
      "valid loss 0.098735 and accuracy 0.984098 \n",
      "training loss:  0.10661250721202807\n",
      "valid loss 0.098992 and accuracy 0.977930 \n",
      "training loss:  0.10427391494612621\n",
      "valid loss 0.099806 and accuracy 0.977737 \n",
      "training loss:  0.10076129898891809\n",
      "valid loss 0.095995 and accuracy 0.984676 \n",
      "training loss:  0.09876784411925633\n",
      "valid loss 0.093054 and accuracy 0.982845 \n",
      "training loss:  0.10269520981563532\n",
      "valid loss 0.098484 and accuracy 0.983809 \n",
      "training loss:  0.110412823271452\n",
      "valid loss 0.099869 and accuracy 0.982267 \n",
      "training loss:  0.10131485151066658\n",
      "valid loss 0.096743 and accuracy 0.985544 \n",
      "training loss:  0.10366746439781319\n",
      "valid loss 0.097338 and accuracy 0.981881 \n",
      "training loss:  0.1009402170097757\n",
      "valid loss 0.098147 and accuracy 0.983712 \n",
      "training loss:  0.09844213326985443\n",
      "valid loss 0.091580 and accuracy 0.984291 \n",
      "training loss:  0.10304390930589744\n",
      "valid loss 0.103866 and accuracy 0.981689 \n",
      "training loss:  0.10380992420160526\n",
      "valid loss 0.099031 and accuracy 0.985158 \n",
      "training loss:  0.10067625624242793\n",
      "valid loss 0.095323 and accuracy 0.983616 \n",
      "training loss:  0.0993332824960932\n",
      "valid loss 0.094564 and accuracy 0.982845 \n",
      "training loss:  0.10466558110739566\n",
      "valid loss 0.095840 and accuracy 0.981785 \n",
      "training loss:  0.10083158932975878\n",
      "valid loss 0.097400 and accuracy 0.982749 \n",
      "training loss:  0.10093540289074242\n",
      "valid loss 0.092399 and accuracy 0.982460 \n",
      "training loss:  0.10019671345487857\n",
      "valid loss 0.095624 and accuracy 0.981881 \n",
      "training loss:  0.10126401372967929\n",
      "valid loss 0.097154 and accuracy 0.984869 \n",
      "training loss:  0.10403854608220607\n",
      "valid loss 0.097541 and accuracy 0.984002 \n",
      "training loss:  0.09859927757954015\n",
      "valid loss 0.093409 and accuracy 0.985062 \n",
      "training loss:  0.0992723137598241\n",
      "valid loss 0.091095 and accuracy 0.986122 \n",
      "training loss:  0.10126472882045001\n",
      "valid loss 0.098289 and accuracy 0.983712 \n",
      "training loss:  0.09857701553279252\n",
      "valid loss 0.097786 and accuracy 0.985158 \n",
      "training loss:  0.09849263426785586\n",
      "valid loss 0.094471 and accuracy 0.983809 \n",
      "training loss:  0.10075505218922265\n",
      "valid loss 0.095044 and accuracy 0.982170 \n",
      "training loss:  0.09810574884573829\n",
      "valid loss 0.095332 and accuracy 0.984580 \n",
      "training loss:  0.1057173505632277\n",
      "valid loss 0.092488 and accuracy 0.984483 \n",
      "training loss:  0.10612618563707822\n",
      "valid loss 0.100472 and accuracy 0.980532 \n",
      "training loss:  0.09907769081376676\n",
      "valid loss 0.092751 and accuracy 0.984387 \n",
      "training loss:  0.10095159214800033\n",
      "valid loss 0.094195 and accuracy 0.983809 \n",
      "training loss:  0.10299484552144611\n",
      "valid loss 0.095388 and accuracy 0.985736 \n",
      "training loss:  0.09952187878175123\n",
      "valid loss 0.099785 and accuracy 0.986604 \n",
      "training loss:  0.10511119578523662\n",
      "valid loss 0.099206 and accuracy 0.985351 \n",
      "training loss:  0.10451150863811187\n",
      "valid loss 0.098110 and accuracy 0.983231 \n",
      "training loss:  0.10061755481681442\n",
      "valid loss 0.096758 and accuracy 0.982170 \n",
      "training loss:  0.09788639215113544\n",
      "valid loss 0.095692 and accuracy 0.982941 \n",
      "training loss:  0.09661507744881308\n",
      "valid loss 0.099502 and accuracy 0.983520 \n",
      "training loss:  0.1051863454508805\n",
      "valid loss 0.096529 and accuracy 0.982363 \n",
      "training loss:  0.10280017618776864\n",
      "valid loss 0.120460 and accuracy 0.984773 \n",
      "training loss:  0.10435062108877788\n",
      "valid loss 0.097644 and accuracy 0.984387 \n",
      "training loss:  0.10191749461285736\n",
      "valid loss 0.099216 and accuracy 0.984580 \n",
      "training loss:  0.0985682026260906\n",
      "valid loss 0.095825 and accuracy 0.977930 \n",
      "training loss:  0.09662111176169241\n",
      "valid loss 0.090672 and accuracy 0.984291 \n",
      "training loss:  0.10503093805225654\n",
      "valid loss 0.092275 and accuracy 0.987182 \n"
     ]
    }
   ],
   "source": [
    "train_loop(model, epochs=1000, lr=0.0001, wd=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以下为贴标签部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目前最好：  98.7953  lr=0.0001 dw=0.05 adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>专利</th>\n",
       "      <th>企业类型</th>\n",
       "      <th>区域</th>\n",
       "      <th>商标</th>\n",
       "      <th>控制人持股比例</th>\n",
       "      <th>控制人类型</th>\n",
       "      <th>注册时间</th>\n",
       "      <th>注册资本</th>\n",
       "      <th>著作权</th>\n",
       "      <th>行业</th>\n",
       "      <th>...</th>\n",
       "      <th>项目融资和政策融资成本</th>\n",
       "      <th>从业人数</th>\n",
       "      <th>资产总额</th>\n",
       "      <th>负债总额</th>\n",
       "      <th>营业总收入</th>\n",
       "      <th>主营业务收入</th>\n",
       "      <th>利润总额</th>\n",
       "      <th>净利润</th>\n",
       "      <th>纳税总额</th>\n",
       "      <th>所有者权益合计</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1080523.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.254808</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.340444</td>\n",
       "      <td>-0.351677</td>\n",
       "      <td>0.184651</td>\n",
       "      <td>0.417659</td>\n",
       "      <td>-0.258273</td>\n",
       "      <td>-0.272971</td>\n",
       "      <td>-0.235634</td>\n",
       "      <td>-0.424045</td>\n",
       "      <td>-0.463028</td>\n",
       "      <td>-0.522521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1080756.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.158117</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.340444</td>\n",
       "      <td>-0.376625</td>\n",
       "      <td>1.042700</td>\n",
       "      <td>1.238097</td>\n",
       "      <td>0.496213</td>\n",
       "      <td>0.527454</td>\n",
       "      <td>1.150685</td>\n",
       "      <td>-0.738470</td>\n",
       "      <td>-0.463028</td>\n",
       "      <td>-0.921111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1080951.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.463926</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.510628</td>\n",
       "      <td>0.342302</td>\n",
       "      <td>0.870810</td>\n",
       "      <td>0.557917</td>\n",
       "      <td>0.176418</td>\n",
       "      <td>0.061825</td>\n",
       "      <td>0.351885</td>\n",
       "      <td>-0.604207</td>\n",
       "      <td>-0.463028</td>\n",
       "      <td>0.077015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1080972.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.58</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.699287</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.340444</td>\n",
       "      <td>0.149991</td>\n",
       "      <td>-0.955436</td>\n",
       "      <td>-0.773011</td>\n",
       "      <td>-0.726476</td>\n",
       "      <td>-0.690639</td>\n",
       "      <td>-0.618036</td>\n",
       "      <td>-0.230269</td>\n",
       "      <td>-0.463028</td>\n",
       "      <td>0.205945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081027.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.231744</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035733</td>\n",
       "      <td>0.421098</td>\n",
       "      <td>-0.210764</td>\n",
       "      <td>-0.388153</td>\n",
       "      <td>-0.282905</td>\n",
       "      <td>-0.202919</td>\n",
       "      <td>-0.323033</td>\n",
       "      <td>-0.415748</td>\n",
       "      <td>-0.463028</td>\n",
       "      <td>0.438726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999614.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.563337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143578</td>\n",
       "      <td>-0.266909</td>\n",
       "      <td>0.724965</td>\n",
       "      <td>-0.034803</td>\n",
       "      <td>0.071062</td>\n",
       "      <td>-0.043789</td>\n",
       "      <td>0.206325</td>\n",
       "      <td>0.250782</td>\n",
       "      <td>0.157708</td>\n",
       "      <td>0.982942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999746.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.91</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.842576</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.340444</td>\n",
       "      <td>-0.485126</td>\n",
       "      <td>-0.465091</td>\n",
       "      <td>-0.262250</td>\n",
       "      <td>-0.274809</td>\n",
       "      <td>-0.346624</td>\n",
       "      <td>-0.239135</td>\n",
       "      <td>0.038662</td>\n",
       "      <td>0.204388</td>\n",
       "      <td>-0.104008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999945.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>-1.646697</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.340444</td>\n",
       "      <td>-0.433419</td>\n",
       "      <td>-0.970887</td>\n",
       "      <td>-0.810904</td>\n",
       "      <td>-0.761767</td>\n",
       "      <td>-0.724710</td>\n",
       "      <td>-0.644649</td>\n",
       "      <td>-0.191496</td>\n",
       "      <td>-0.449280</td>\n",
       "      <td>0.255177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999952.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-0.447639</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.340444</td>\n",
       "      <td>0.306643</td>\n",
       "      <td>-0.441269</td>\n",
       "      <td>-0.382515</td>\n",
       "      <td>-0.502252</td>\n",
       "      <td>-0.397663</td>\n",
       "      <td>-0.367992</td>\n",
       "      <td>-0.121098</td>\n",
       "      <td>-0.325291</td>\n",
       "      <td>0.136300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999964.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.488468</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.247776</td>\n",
       "      <td>0.768044</td>\n",
       "      <td>-0.243172</td>\n",
       "      <td>0.096035</td>\n",
       "      <td>-0.408597</td>\n",
       "      <td>-0.447744</td>\n",
       "      <td>-0.324118</td>\n",
       "      <td>0.237529</td>\n",
       "      <td>-0.032559</td>\n",
       "      <td>-0.474952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10003 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            专利 企业类型 区域   商标 控制人持股比例 控制人类型  注册时间      注册资本  著作权 行业  ...  \\\n",
       "ID                                                                 ...   \n",
       "1080523.0  0.0    0  1  0.0    0.57     0   0.0 -0.254808  0.0  4  ...   \n",
       "1080756.0  0.0    0  0  0.0    0.79     0   5.0  1.158117  0.0  5  ...   \n",
       "1080951.0  1.0    2  4  1.0    0.93     0   4.0  0.463926  0.0  2  ...   \n",
       "1080972.0  0.0    4  4  0.0    0.58     1   0.0 -1.699287  0.0  1  ...   \n",
       "1081027.0  1.0    1  6  1.0    0.72     1  12.0  1.231744  0.0  5  ...   \n",
       "...        ...  ... ..  ...     ...   ...   ...       ...  ... ..  ...   \n",
       "5999614.0  1.0    4  0  1.0    0.51     0   2.0 -0.563337  0.0  2  ...   \n",
       "5999746.0  0.0    0  3  0.0    0.91     1  11.0  0.842576  0.0  2  ...   \n",
       "5999945.0  0.0    3  4  1.0    0.83     0  13.0 -1.646697  1.0  4  ...   \n",
       "5999952.0  1.0    0  3  1.0    0.62     0   6.0 -0.447639  1.0  5  ...   \n",
       "5999964.0  0.0    0  0  0.0    0.72     1  10.0  0.488468  1.0  2  ...   \n",
       "\n",
       "          项目融资和政策融资成本      从业人数      资产总额      负债总额     营业总收入    主营业务收入  \\\n",
       "ID                                                                        \n",
       "1080523.0   -0.340444 -0.351677  0.184651  0.417659 -0.258273 -0.272971   \n",
       "1080756.0   -0.340444 -0.376625  1.042700  1.238097  0.496213  0.527454   \n",
       "1080951.0    0.510628  0.342302  0.870810  0.557917  0.176418  0.061825   \n",
       "1080972.0   -0.340444  0.149991 -0.955436 -0.773011 -0.726476 -0.690639   \n",
       "1081027.0   -0.035733  0.421098 -0.210764 -0.388153 -0.282905 -0.202919   \n",
       "...               ...       ...       ...       ...       ...       ...   \n",
       "5999614.0    0.143578 -0.266909  0.724965 -0.034803  0.071062 -0.043789   \n",
       "5999746.0   -0.340444 -0.485126 -0.465091 -0.262250 -0.274809 -0.346624   \n",
       "5999945.0   -0.340444 -0.433419 -0.970887 -0.810904 -0.761767 -0.724710   \n",
       "5999952.0   -0.340444  0.306643 -0.441269 -0.382515 -0.502252 -0.397663   \n",
       "5999964.0   -0.247776  0.768044 -0.243172  0.096035 -0.408597 -0.447744   \n",
       "\n",
       "               利润总额       净利润      纳税总额   所有者权益合计  \n",
       "ID                                                 \n",
       "1080523.0 -0.235634 -0.424045 -0.463028 -0.522521  \n",
       "1080756.0  1.150685 -0.738470 -0.463028 -0.921111  \n",
       "1080951.0  0.351885 -0.604207 -0.463028  0.077015  \n",
       "1080972.0 -0.618036 -0.230269 -0.463028  0.205945  \n",
       "1081027.0 -0.323033 -0.415748 -0.463028  0.438726  \n",
       "...             ...       ...       ...       ...  \n",
       "5999614.0  0.206325  0.250782  0.157708  0.982942  \n",
       "5999746.0 -0.239135  0.038662  0.204388 -0.104008  \n",
       "5999945.0 -0.644649 -0.191496 -0.449280  0.255177  \n",
       "5999952.0 -0.367992 -0.121098 -0.325291  0.136300  \n",
       "5999964.0 -0.324118  0.237529 -0.032559 -0.474952  \n",
       "\n",
       "[10003 rows x 27 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_label=encoded_all_data.drop(init_x.index)\n",
    "non_label_X=non_label.drop(['flag'],axis=1)\n",
    "estimate_y=estimate_y=np.array([1]*non_label_X.shape[0])\n",
    "non_label_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making all variables categorical\n",
    "for col in non_label.columns:\n",
    "    non_label[col] = non_label[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'行业': 6, '企业类型': 5, '控制人类型': 2, '区域': 7}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#categorical embedding for columns having more than two values\n",
    "embedded_cols = {n: len(col.cat.categories) for n,col in txt.items()}\n",
    "embedded_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, 3), (5, 3), (2, 1), (7, 4)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_sizes = [(n_categories, min(50, (n_categories+1)//2)) for _,n_categories in embedded_cols.items()]\n",
    "embedding_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = CompanyDataset(non_label_X,estimate_y,embedded_col_names)\n",
    "batch_size2 = 10275\n",
    "test_dl = DataLoader(test_ds,batch_size=batch_size2,shuffle=False) #返回的是可迭代对象\n",
    "# first=iter(test_dl)\n",
    "# next(first)\n",
    "test_dl = DeviceDataLoader(test_dl, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_dl):\n",
    "    model.eval()\n",
    "    for x1, x2, y in test_dl:\n",
    "        out = model(x1, x2)\n",
    "        pred = torch.max(out, 1)[1]\n",
    "        return pred.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flag=predict(model,test_dl)\n",
    "flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_final=pd.concat([pd.DataFrame(index=non_label_X.index,data=flag,columns=['flag']),],axis=1)\n",
    "neuron_final=encoded_all_data.combine_first(neuron_final)\n",
    "neuron_final.to_csv(r'./data/created_data/neuron_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 19987 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 21033 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 20027 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 33829 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 19994 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 21153 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 25910 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 20837 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 20174 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 20154 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 25968 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 20225 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 31867 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 22411 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 20538 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 26435 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 34701 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 36164 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 25104 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 26412 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 39069 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 24230 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 20869 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 37096 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 21644 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 36152 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 26131 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 20928 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 28070 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 24635 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 21306 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 22495 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 21830 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 26631 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 25152 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 26377 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 32773 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 30410 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 21512 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 35745 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 25511 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 21046 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 25345 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 32929 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 27604 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 20363 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 27880 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 20876 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 26102 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 38388 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 32435 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 31246 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 33879 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 20316 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 34892 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 36127 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 20135 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 39033 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 30446 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 25919 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:90: RuntimeWarning: Glyph 31574 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 19987 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 21033 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 20027 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 33829 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 19994 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 21153 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 25910 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 20837 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 20174 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 20154 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 25968 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 20225 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 31867 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 22411 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 20538 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 26435 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 34701 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 36164 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 25104 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 26412 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 39069 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 24230 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 20869 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 37096 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 21644 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 36152 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 26131 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 20928 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 28070 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 24635 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 21306 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 22495 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 21830 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 26631 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 25152 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 26377 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 32773 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 30410 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 21512 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 35745 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 25511 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 21046 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 25345 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 32929 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 27604 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 20363 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 27880 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 20876 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 26102 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 38388 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 32435 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 31246 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 33879 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 20316 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 34892 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 36127 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 20135 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 39033 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 30446 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 25919 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\textpath.py:203: RuntimeWarning: Glyph 31574 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 19987 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 21033 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 20027 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 33829 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 19994 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 21153 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 25910 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 20837 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 20174 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 20154 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 25968 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 20225 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 31867 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 22411 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 20538 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 26435 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 34701 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 36164 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 25104 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 26412 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 39069 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 24230 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 20869 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 37096 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 21644 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 36152 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 26131 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 20928 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 28070 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 24635 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 21306 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 22495 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 21830 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 26631 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 25152 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 26377 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 32773 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 30410 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 21512 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 35745 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 25511 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 21046 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 25345 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 32929 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 27604 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 20363 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 27880 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 20876 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 26102 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 38388 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 32435 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 31246 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 33879 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 20316 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 34892 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 36127 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 20135 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 39033 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 30446 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 25919 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 31574 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 19987 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 21033 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 20027 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 33829 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 19994 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 21153 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 25910 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 20837 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 20174 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 20154 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 25968 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 20225 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 31867 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 22411 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 20538 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 26435 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 34701 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 36164 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 25104 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 26412 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 39069 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 24230 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 20869 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 37096 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 21644 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 36152 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 26131 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 20928 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 28070 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 24635 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 21306 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 22495 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 21830 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 26631 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 25152 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 26377 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 32773 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 30410 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 21512 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 35745 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 25511 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 21046 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 25345 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 32929 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 27604 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 20363 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 27880 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 20876 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 26102 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 38388 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 32435 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 31246 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 33879 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 20316 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 34892 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 36127 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 20135 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 39033 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 30446 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 25919 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\chenhongda\\Anaconda3\\envs\\aikedaer\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 31574 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e912dc0379e4a6a83b0ca1d88e1c16f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(HTML(value='<div id=\"overview-content\" class=\"row variable spacing\">\\n    <div class=\"row\">\\n   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Report generated with <a href=\"https://github.com/pandas-profiling/pandas-profiling\">pandas-profiling</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppf.ProfileReport(neuron_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
